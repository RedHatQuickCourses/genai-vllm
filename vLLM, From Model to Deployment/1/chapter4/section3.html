<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>vLLM Serving Runtime on Red Hat OpenShift AI :: vLLM, From Model to Deployment</title>
    <link rel="prev" href="section1.html">
    <link rel="next" href="../appendix/appendix.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">vLLM, From Model to Deployment</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="vLLM, From Model to Deployment" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">vLLM, From Model to Deployment</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/index.html">Red Hat AI</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/page1.html">Red Hat AI Platforms</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/section1.html">The Challenge</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/section2.html">The Scenario</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/openshift_ai_overview.html">Innovatech&#8217;s OpenShift AI Environment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/section3.html">Red Hat AI Platform Solutions</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/section4.html">Model Inference with OpenShift AI</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Chapter 2</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/section1.html">Course Module: Real-World Use Case - The Support Ticket Triage Assistant</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter3/index.html">Red Hat AI Model Repository</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/section1.html">Red Hat AI: Validated Models and Quantization Strategies</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/section2.html">Individual Model Analysis: Pre-Selected for This Course</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/section3.html">Course Module: Real-World Use Case - The Support Ticket Triage Assistant</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">vLLM, What is it ?</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section2.html">Introduction to vLLM</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section1.html">vLLM Runtime</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="section3.html">vLLM Serving Runtime on Red Hat OpenShift AI</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/model_phases.html">Common Generative AI Model Variations</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/further_study.html">Hands on Exploration and Further Study</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../LABENV/index.html">Lab Environments</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/index-lab-demo.html">Red Hat Demo Hub: Lab Environment Selection</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/developer_sandbox.html">Red Hat Developer Platform</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">vLLM, From Model to Deployment</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">vLLM, From Model to Deployment</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">vLLM, From Model to Deployment</a></li>
    <li><a href="index.html">vLLM, What is it ?</a></li>
    <li><a href="section3.html">vLLM Serving Runtime on Red Hat OpenShift AI</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">vLLM Serving Runtime on Red Hat OpenShift AI</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>This section details how to integrate and utilize the vLLM inference engine as a serving runtime within the Red Hat OpenShift AI platform&#8217;s Single-Model Serving stack. This allows you to deploy Large Language Models (LLMs) with vLLM&#8217;s high-performance capabilities, offering an alternative to standard runtimes like Caikit+TGIS or standalone TGIS.</p>
</div>
<div class="paragraph">
<p>For a comprehensive list of models currently supported by vLLM, refer to the official documentation: <a href="https://docs.vllm.ai/en/latest/models/supported_models.html" target="_blank" rel="noopener">vLLM Supported Models</a>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="gpu-cpu-considerations"><a class="anchor" href="#gpu-cpu-considerations"></a>1. GPU vs. CPU Considerations</h2>
<div class="sectionbody">
<div class="paragraph">
<p>While vLLM is primarily optimized for GPU acceleration, offering custom CUDA kernels for maximum throughput, it also provides a CPU-only version for scenarios where GPUs are unavailable or for specific development and testing needs.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>GPU-Accelerated vLLM (Recommended for Production):</strong></p>
<div class="ulist">
<ul>
<li>
<p><strong>Requirement:</strong> A <strong>GPU is mandatory</strong> to load and run models efficiently due to its reliance on custom CUDA kernels.</p>
</li>
<li>
<p><strong>Performance:</strong> Designed for high-throughput and low-latency inference, leveraging the parallel processing power of GPUs.</p>
</li>
<li>
<p><strong>Memory:</strong> Requires sufficient GPU VRAM (Video RAM) to load the model weights. Note that standard RAM is still utilized for pre-loading operations before the model resides fully on the GPU.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>CPU-Only vLLM (For Specific Use Cases):</strong></p>
<div class="ulist">
<ul>
<li>
<p><strong>Requirement:</strong> No GPU needed; models run entirely on the CPU.</p>
</li>
<li>
<p><strong>Limitations:</strong></p>
<div class="ulist">
<ul>
<li>
<p><strong>Performance:</strong> Generally slower than GPU-accelerated vLLM, suitable for smaller models, development, or specific non-performance-critical scenarios.</p>
</li>
<li>
<p><strong>Data Type Compatibility:</strong> The CPU version currently has limitations regarding supported data types. It is primarily compatible with FP32 and BF16. Any FP16 model will be automatically converted to BF16 upon loading. For detailed information on CPU limitations and supported features, consult the official vLLM CPU installation documentation: <a href="https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html" target="_blank" rel="noopener">vLLM CPU Installation</a>.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_2_installation_and_configuration"><a class="anchor" href="#_2_installation_and_configuration"></a>2. Installation and Configuration</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Before adding the vLLM runtime, ensure that the necessary components of the Red Hat OpenShift AI Single-Model Serving stack are properly installed and configured. Refer to the official Red Hat documentation for this prerequisite: <a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2-latest/html/serving_models/serving-large-models_serving-large-models" target="_blank" rel="noopener">Serving Large Models on OpenShift AI</a>.</p>
</div>
<div class="paragraph">
<p>Once the Single-Model Serving stack is ready, adding the vLLM serving runtime is a straightforward process for an administrator:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>As an administrator, log in to the OpenShift AI Dashboard.</p>
</li>
<li>
<p>Navigate to <code>Settings</code> &#8594; <code>Serving runtimes</code> in the left-hand menu.</p>
</li>
<li>
<p>Click on the <code>Add serving runtime</code> button.</p>
</li>
<li>
<p>For the "Type of model serving platforms this runtime supports," select <code>Single model serving platform</code>.</p>
</li>
<li>
<p>You have two options to define the runtime configuration:</p>
<div class="ulist">
<ul>
<li>
<p><strong>Upload File:</strong> Upload the provided <code>vllm-runtime.yaml</code> file from your current working directory.</p>
</li>
<li>
<p><strong>Start from scratch:</strong> Click <code>Start from scratch</code> and copy/paste the content of the <code>vllm-runtime.yaml</code> file into the editor.</p>
</li>
<li>
<p>[NOTE]
A dedicated CPU-only version of the runtime is typically available in a separate YAML file (e.g., <code>vllm-runtime-cpu.yaml</code>), which should be used for CPU deployments.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>The vLLM serving runtime will now be available for selection when deploying a new model.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_3_model_deployment_with_vllm"><a class="anchor" href="#_3_model_deployment_with_vllm"></a>3. Model Deployment with vLLM</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Deploying a model using the vLLM runtime follows the standard procedures within the OpenShift AI Dashboard:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Prepare Model Files:</strong> Ensure your trained LLM files are copied to an accessible S3-compatible object storage bucket.</p>
</li>
<li>
<p><strong>Initiate Deployment:</strong> From the OpenShift AI Dashboard, navigate to your project and initiate a new model deployment.</p>
</li>
<li>
<p><strong>Select Runtime:</strong> During the deployment process, select the <code>vLLM</code> serving runtime that you added in the previous step.</p>
</li>
<li>
<p><strong>Resource Allocation:</strong></p>
<div class="ulist">
<ul>
<li>
<p><strong>GPU Deployments:</strong> Verify that you have configured a GPU for your deployment and allocated sufficient <strong>GPU VRAM</strong> to accommodate the model. Also, ensure adequate <strong>standard memory (RAM)</strong> is provisioned, as it&#8217;s used for initial model pre-loading operations before the model is fully resident on the GPU.</p>
</li>
<li>
<p><strong>CPU Deployments:</strong> For CPU-only deployments, ensure sufficient standard RAM and CPU resources are allocated. Be mindful of the CPU-specific data type limitations mentioned in Section 1.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Monitor Deployment:</strong> Monitor the deployment status from the dashboard. Once the model is successfully loaded and the serving instance is ready, the inference endpoint will become accessible.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_4_consuming_the_vllm_inference_endpoint"><a class="anchor" href="#_4_consuming_the_vllm_inference_endpoint"></a>4. Consuming the vLLM Inference Endpoint</h2>
<div class="sectionbody">
<div class="paragraph">
<p>One of vLLM&#8217;s significant advantages for application integration is its <strong>OpenAI-compatible API</strong>. This means any tool or library capable of connecting to OpenAI services can seamlessly consume the vLLM inference endpoint.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>API Endpoints:</strong>
vLLM&#8217;s OpenAI-compatible server implements key OpenAI API endpoints, including:</p>
<div class="ulist">
<ul>
<li>
<p><code>/v1/completions</code> (for text generation)</p>
</li>
<li>
<p><code>/v1/chat/completions</code> (for conversational models)</p>
</li>
<li>
<p><code>/v1/embeddings</code> (for embedding models)</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Development Examples:</strong></p>
<div class="ulist">
<ul>
<li>
<p><strong>Official Quickstart:</strong> Refer to the vLLM documentation for Python and <code>curl</code> examples demonstrating how to interact with the OpenAI-compatible API: <a href="https://docs.vllm.ai/en/latest/getting_started/quickstart.html#using-openai-completions-api-with-vllm" target="_blank" rel="noopener">vLLM OpenAI Quickstart</a>.</p>
</li>
<li>
<p><strong>LangChain Integration:</strong> A notebook example demonstrating how to query a vLLM endpoint using the LangChain framework is available in this repository: <code>../../examples/notebooks/langchain/Langchain-vLLM-Prompt-memory.ipynb</code>.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Interactive API Documentation (Swagger UI):</strong>
For direct API exploration and testing without writing code, vLLM provides a built-in Swagger UI. It offers comprehensive documentation of all API methods, parameters, and allows you to test endpoints directly from your browser.</p>
<div class="ulist">
<ul>
<li>
<p><strong>Access:</strong> This UI is typically accessible at the address: <code><a href="https://your-model-inference-endpoint-address/docs" class="bare">https://your-model-inference-endpoint-address/docs</a></code>. (Replace <code>your-model-inference-endpoint-address</code> with the actual endpoint URL provided by the OpenShift AI Dashboard for your deployed model.)</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="section1.html">vLLM Runtime</a></span>
  <span class="next"><a href="../appendix/appendix.html">Appendix</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
