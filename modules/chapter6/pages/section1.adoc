= Course: High-Performance LLM Serving with vLLM
:toc:
:toclevels: 2
:sectnums:

== Module 4: The Curated Model Repository

=== Introduction: From the "Wild West" to a Validated Ecosystem

Welcome to Module 4. In previous modules, we learned how to serve models efficiently with vLLM and how to select hardware based on cost and performance features. Now, we address a critical question: **Which model should you choose?**

The open-source AI landscape, particularly on platforms like Hugging Face, can feel like the "Wild West"â€”a vast space with thousands of models, making it difficult to know which ones are performant, accurate, or ready for enterprise use.

This module introduces the **Red Hat AI Validated Model Repository**, a curated ecosystem designed to solve this problem. You will learn how to leverage this resource to de-risk your projects, accelerate deployment, and make informed decisions based on transparent, rigorous testing.

---

=== 4.1. The Red Hat AI Validated Model Program

This first lesson explains the "why" behind the Red Hat AI initiative and the tangible benefits of using a validated model.

==== The Open-Source AI Initiative

The Red Hat AI repository on Hugging Face is an open-source initiative born from a deep collaboration between IBM and Red Hat. Its mission is to bridge the gap between cutting-edge research and robust production deployments by providing open, accessible, and community-driven AI solutions.

By sharing models, research, and validated configurations, the program empowers you to deploy high-performance AI at scale with confidence.

==== Why Use a Validated Model?

A "validated model" from the Red Hat AI repository is more than just a pre-trained LLM. It's a model that has been meticulously assessed through comprehensive testing, providing critical insights into its real-world behavior.

[IMPORTANT]
.Key Benefits of Using a Validated Model
****
* **Reduces Guesswork:** Provides reliable performance expectations and recommended deployment settings, eliminating the need for extensive trial-and-error.
* **Ensures Enterprise Readiness:** Validates that models perform well under load using enterprise-grade tools like vLLM.
* **Optimizes for Cost:** Offers clear guidance on pairing models with the right hardware, maximizing inference efficiency and lowering Total Cost of Ownership (TCO).
* **Provides Flexibility:** Offers a range of popular models in various optimized formats to meet diverse project requirements.
****

==== The Validation Process

Red Hat's validation process uses industry-standard open-source tooling to ensure the results are transparent and reproducible:

* **`GuideLLM`:** A powerful benchmarking tool used for capacity planning. It simulates real-world traffic to assess a model's throughput, latency, and resource utilization on various hardware setups.
* **`Language Model Evaluation Harness` (LM Eval Harness):** The industry standard for accuracy evaluation. It measures a model's reasoning and generalization capabilities across a wide range of academic benchmarks.
* **`vLLM`:** The inference engine used during performance validation. This ensures that the published benchmarks reflect the performance you can expect when deploying with the tools taught in this course.

---

=== 4.2. A Deep Dive into Quantization Strategies

This lesson focuses on quantization, the core optimization technique used for the models in the Red Hat AI repository.

==== Understanding the Naming Convention

When Browse the repository, you will see suffixes like `.w4a16` or `.FP8-dynamic`. This is a standardized shorthand that tells you the precision of the model's **weights (w)** and **activations (a)**.

* **`w4a16`**: The model's **w**eights are quantized to 4-bit integers, and the **a**ctivations (the in-flight calculations) are processed at 16-bit precision (FP16/BF16). This is a very common "weight-only" quantization scheme.
* **`w8a8`**: Both weights and activations are processed at 8-bit integer precision.
* **`FP8` or `FP8-dynamic`**: The model leverages the 8-bit floating-point format, often with dynamic scaling to maintain accuracy.

==== The Performance vs. Accuracy Trade-Off

No single quantization level is best for every situation. The key is to balance performance, accuracy, and cost for your specific use case.

.Quantization Strategy Comparison
[options="header"]
|===
| Precision | Best For | Hardware Alignment | Accuracy Risk

| **FP16 / BF16**
| Highest accuracy, baseline performance.
| All modern GPUs.
| **Lowest (None)**

| **FP8**
| **Excellent balance of speed and accuracy.**
| Natively accelerated on **Hopper (H100), Ada (L4/L40S), and Blackwell (B100)** GPUs.
| **Low.** Generally preserves accuracy better than integer formats.

| **INT8**
| Good speed and memory savings on a wide range of hardware.
| Widely supported, but most performant on GPUs with INT8 Tensor Cores.
| **Medium.** Can cause accuracy degradation in some models; requires testing.

| **INT4**
| **Maximum memory reduction.** Fitting large models on smaller GPUs.
| All modern GPUs, but performance benefits depend on efficient software implementation (e.g., via vLLM).
| **Highest.** Requires advanced quantization techniques (like AWQ/GPTQ) to mitigate accuracy loss.
|===

[TIP]
As discussed in the previous module, your quantization choice should be **hardware-aware**. If your target is an H100 or L40S GPU, an `FP8` quantized model is the most strategic choice as you are leveraging a feature the hardware was specifically designed to accelerate.

---

=== 4.3. Navigating and Selecting a Validated Model

This final lesson guides you through the model collections and the criteria used to select them, empowering you to make smart choices for your own projects.

==== Enterprise Selection Criteria

The models in the validated collection are not chosen at random. They are selected based on criteria that reflect real-world enterprise constraints and goals, including:

* **Performance on Cost-Effective Hardware:** Can the model run effectively on an NVIDIA L4 or A10G GPU (24GB VRAM)?
* **Total Cost of Ownership (TCO):** Does the deployment align with a reasonable yearly infrastructure budget (e.g., < $75,000)?
* **Demonstrated Business Impact:** Can the model solve tangible business problems, like improving developer productivity or automating customer support?

==== Pre-Selected Models for This Course

For the hands-on labs in this course, we have pre-selected a representative sample from the Red Hat AI validated repository. These models showcase different sizes, architectures, and quantization strategies, giving you a broad range of experience.

* `RedHatAI/Mistral-7B-Instruct-v0.3-quantized.w4a16`
* `RedHatAI/Qwen2-7B-Instruct-quantized.w8a8`
* `RedHatAI/gemma-2-9b-it-quantized.FP8-dynamic`

These models provide a perfect starting point for exploring the trade-offs between a 4-bit weight quantized model, a full 8-bit model, and a modern FP8 model.

#### Next Steps

In the following labs, we will dive deep into these three models. You will deploy each one using vLLM on OpenShift AI, benchmark their performance, and evaluate their outputs, giving you the practical experience needed to select the right model for your next project.