= Deploying the Granite Model with vLLM

In the previous sections, we prepared our environment, configured storage, and uploaded the `granite-3.3-2B-instruct` model. Now, it's time to deploy that model as a live inference service using the vLLM runtime.

This guide will walk you through each setting in the **Single-Model Serving** deployment form to ensure a successful launch.

---

== Initiating the Model Deployment

First, let's navigate to the model deployment screen within our project.

1.  From the Red Hat OpenShift AI dashboard, ensure you are inside your Data Science Project (`vllm-lab`).
2.  Click the **Deploy models** button.

You will be presented with two options: Single-model serving and Multi-model serving. For this lab, we will use the single-model option.

=== Step 1: Model Deployment Name

This name identifies your deployment within the project and is also used as the internal service name for the model.

* **Model deployment name:** Enter `granite-model`.

[NOTE]
====
When other applications inside the cluster connect to this model, they will use this service name (e.g., `granite-model`). The full Hugging Face model name is not used for the service endpoint.
====

=== Step 2: Serving Runtime and Framework

Here, you select the pre-configured environment that will serve the model.

* **Serving runtime:** From the dropdown menu, select the **vLLM (NVIDIA GPU)** runtime.
* **Model framework:** This field should automatically populate with `vLLM` after selecting the runtime.

=== Step 3: Number of Replicas

This setting controls the high availability and scalability of your model. Each replica runs in its own pod and requires its own dedicated GPU.

* **Number of model replicas:** Keep this value set to **1**. For a lab environment, a single replica is sufficient.

=== Step 4: Model Server Size

This determines the amount of CPU and RAM allocated to the server pod. While the GPU handles the core inference computation, the CPU and RAM are critical for loading the model and handling requests.

* **Model server size:** Select **Large**. LLMs are memory-intensive, and providing ample RAM helps prevent out-of-memory errors during the model loading process.

=== Step 5: External Access and Authentication

These settings control how your model is exposed and secured.

.   Check the box labeled **Make deployed model available through an external route**. This creates a public URL so applications outside the cluster can access your model.
.   Notice that **Require token authentication** is automatically selected. This is a critical security feature that ensures only authorized clients can use the model.
.   A new field for a **Service account** will appear. For this lab, you can leave the `default` service account selected.

=== Step 6: Model Location

This is the most critical step, where you tell the server where to find the model files.

.   Ensure **Existing data connection** is selected.
.   From the dropdown, choose the **models-storage** data connection we created earlier.
.   In the **Folder name** text box, you must provide the exact name of the subdirectory where the Granite model files reside. Enter: `granite-3b-code-base`

=== Step 7: Deploy the Model

You can leave the optional "Serving runtime arguments" blank for this deployment.

* Click the **Deploy** button at the bottom of the page.

The deployment process has now started. It can take a significant amount of time (10-20 minutes or more) as the system first provisions the pod and then downloads the multi-gigabyte model files from your MinIO S3 storage into the container.

You can monitor the progress on the **Models and model servers** page of your project. Wait for the status to show as **Running** before proceeding.

---

== Interactive Demo: Deploying the Granite LLM

The following interactive demonstration will walk you through the process you just read about: deploying an LLM using the Single-Model Serving user interface in Red Hat OpenShift AI.

++++
<iframe
  src="https://demo.arcade.software/uIewLOxprc7Sw6LFRAzJ?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true"
  width="100%"
  height="600px"
  frameborder="0"
  allowfullscreen
  webkitallowfullscreen
  mozallowfullscreen
  allow="clipboard-write"
  muted>
</iframe>
++++
