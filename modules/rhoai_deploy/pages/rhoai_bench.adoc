= Creating the AI Workbench and Preparing Models

With our project and data connections configured, it's time to create our primary development environment: the **Red Hat OpenShift AI Workbench**. 


== Part 1: Configure and Launch the Workbench

The workbench provides a fully containerized and interactive data science environment, most with JupyterLab as as the core component. In this segment, we will select the right tools, compute resources, and storage connections to build and test our vLLM deployments.

From the Data Science Project dashboard, click **Create workbench**.

 .  **Name:** Provide a descriptive name for your workbench, for example, `vllm-workbench`.

 .  **Notebook image:** This is the container image that defines your development environment. 

    * Scroll through the list of available images. You will see options like Standard Python, PyTorch, and TensorFlow.

 .  **Container size:** Select the amount of CPU and Memory allocated to your workbench.
    * The default sizes are Small, Medium, and Large. For this lab, a **Small** or **Medium** instance is sufficient.
    * (Example: Small - 2 CPU, 8 GB RAM; Medium - 4 CPU, 16 GB RAM)

 .  **Accelerator:** allocate a GPU to the workbench.
    * We won't need an accelerator for this lab
  
 .  **Data Connections:** Connect the workbench to the S3 storage we configured earlier.
    * Select the *models* connection from the dropdown box for data connections. This will mount our `models` storage bucket inside the workbench, for easy access.

 .  **Cluster storage:** This is the persistent storage for your workbench's home directory. Leave the default settings to create a new persistent volume.

8.  Review your selections and click **Create workbench**.

The workbench will now begin to provision. This process can take several minutes. While it is starting, we can proceed with the next part of our setup.




---



== Part 2: Download and Upload AI Models

When deploying a model server in OpenShift AI, the most common method is to have it download the model files directly from an S3-compatible storage location. 

The model used in this lab is:  https://huggingface.co/ibm-granite/granite-3.3-2b-instruct[ibm-granite/granite-3.3-2b-instruct, window=blank]

```bash
git clone --depth=1 git clone https://huggingface.co/ibm-granite/granite-3.3-2b-instruct
```

=== Step A: Download Models from Hugging Face

On your local computer, create a directory for your models. The easiest way to download a model from Hugging Face is using `git`.


=== Step B: Upload Models to MinIO

It is a requirementâ€”that model files reside in a subdirectory within your storage bucket, not in the root. Create a folder for each model.

 * Log back into the MinIO UI dashboard.

* Navigate to the models bucket.

For each model you downloaded, perform the following actions:

 .  Click *Create new path* to create a subdirectory named after the model (e.g., granite).
 .  Click the Upload button and select the AI model files from local foldel.
 ..  Upload all of its contents, not just the model files. 



== Interactive Demo: Project, Deployment, and Data Connection Walkthrough


++++
<iframe
  src="https://demo.arcade.software/o95APuRqMKhyFEkEFXhf?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true"
  width="100%"
  height="600px"
  frameborder="0"
  allowfullscreen
  webkitallowfullscreen
  mozallowfullscreen
  allow="clipboard-write"
  muted>
</iframe>
++++




