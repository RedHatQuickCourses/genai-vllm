= Integrating vLLM with OpenShift AI: The Serving Runtime

This section focuses on the fundamental concept of a Serving Runtime, particularly the vLLM Serving Runtime, which is crucial for a delivery engineer to understand for configuration and troubleshooting.


== What is a Serving Runtime?

In the context of OpenShift AI and its underlying model-serving framework (KServe), a Serving Runtime is a **reusable template or blueprint** that defines a specific environment for serving a model.


Think of it as a standardized recipe for deploying a model server. Instead of configuring a container image, startup commands, and resource requests for every single model, you define them once in a runtime. When you deploy a model, you simply tell OpenShift AI: "Use this runtime."

This abstraction is a *core MLOps principle*. It separates the *model artifact* (the "what") from the *serving environment* (the "how"), leading to standardized, repeatable, and maintainable deployments.

****
A Serving Runtime specifies critical information, such as:

 * The **container image** that contains the model server (e.g., the official vLLM image).
 * The **command and arguments** needed to start the server and load a model.
 * **Hardware requirements**, especially the need for specific accelerators like NVIDIA GPUs.
 * **Network ports** and paths for inference requests and metrics collection (Prometheus).
 * **Supported model formats** that the runtime can handle.
****


== The vLLM Serving Runtime for Red Hat OpenShift AI

By creating a vLLM ServingRuntime, vLLM becomes a first-class citizen in the RHOAI model deployment workflow, serving as a high-performance alternative to other runtimes like Caikit+TGIS.

[IMPORTANT]
====
*GPU Requirement*

The standard vLLM runtime is built on CUDA kernels for maximum performance, making a GPU mandatory on the OpenShift AI worker nodes where the model will be deployed. While an experimental CPU version exists, production use cases focus exclusively on GPU-based serving.
====

=== Anatomy of the `vllm-runtime.yaml`

To register vLLM with OpenShift AI, an administrator applies a YAML file defining the ServingRuntime resource. Understanding the fields within this YAML is critical for customizing, optimizing, and troubleshooting LLM serving environments. Let's break down an example to understand its key components.

.vLLM ServingRuntime for NVIDIA GPUs
[%collapsible]
====
[source,yaml,linenums]
----
# Filename: vllm-servingruntime.yaml
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
 name: vllm-cuda-runtime-example
 annotations:
   openshift.io/display-name: vLLM (NVIDIA GPU)
   opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
 labels:
   opendatahub.io/dashboard: 'true'
spec:
 supportedModelFormats:
   - name: vLLM
     autoSelect: true
 containers:
   - name: kserve-container
     image: quay.io/modh/vllm:rhoai-2.20-cuda
     command: ["python", "-m", "vllm.entrypoints.openai.api_server"]
     args:
       - "--port=8080"
       - "--model=/mnt/models"
       - "--served-model-name={{.Name}}"
     env:
       - name: HF_HOME
         value: /tmp/hf_home
     ports:
       - containerPort: 8080
         protocol: TCP
     resources:
       requests:
         nvidia.com/gpu: '1'
       limits:
         nvidia.com/gpu: '1'
----
====

.Key Fields Explained
[cols="1,3"]
|===
|Field (Line #) | Description

|`metadata.name` (4)
|A unique name for the runtime within the namespace.

|`metadata.annotations` (5-7)
|Crucial for UI integration. `openshift.io/display-name` sets the friendly name shown in the OpenShift AI dashboard. `opendatahub.io/recommended-accelerators` signals to the platform that this runtime requires an `nvidia.com/gpu`.

|`spec.supportedModelFormats` (11-13)
|Declares which model formats this runtime supports. `autoSelect: true` allows OpenShift AI to automatically pick this runtime for compatible models.

|`spec.containers.image` (15)
|The specific container image to run. Here, it points to an official image for vLLM on RHOAI. For AMD GPUs, this would be a ROCm image.

|`spec.containers.args` (17-19)
|The command-line arguments passed to the vLLM server at startup.
*`--port=8080`*: Exposes the server on this port.
*`--model=/mnt/models`*: This is critical. It tells vLLM to load the model from the `/mnt/models` directory, which is the standard location where OpenShift AI's serving platform mounts model data.
*`--served-model-name={{.Name}}`*: A template variable that passes the deployed model's name to the server.

|`spec.containers.resources` (25-29)
|Defines the resource requests and limits. Requesting and limiting `nvidia.com/gpu: '1'` ensures the pod is scheduled on a GPU-equipped node and is allocated one GPU.
|===

