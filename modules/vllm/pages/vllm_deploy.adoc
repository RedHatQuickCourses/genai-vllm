= Deploying and Interacting with Models on OpenShift AI

This page details the practical, step-by-step workflow for deploying and interacting with models, emphasizing resource allocation which is vital for delivery engineers.


== The Model Deployment Workflow (End-to-End):

 . *Register the Runtime (Admin Task):* 
An administrator with cluster privileges must first install necessary OpenShift AI components. Then, they add the vLLM runtime by navigating to the OpenShift AI Dashboard, going to Settings → Serving runtimes, clicking Add serving runtime, choosing Single model serving platform, and pasting the vllm-servingruntime.yaml content. The "vLLM (NVIDIA GPU)" runtime will then be available.

 . *Prepare Model Artifacts:* Model files (e.g., from Hugging Face) must be located in an S3-compatible object storage bucket that OpenShift AI can access. A Data Connection pointing to this bucket must be created in the OpenShift AI project.

 . *Deploy a Model (User Task):*
 .. From the OpenShift AI project dashboard, click Deploy model.
 .. Fill in the model name, select the vLLM (NVIDIA GPU) serving runtime, and point to the data connection and specific model path.
 .. Critical Resource Allocation: This is the most critical step for successful deployment.
 ... GPU VRAM (Video RAM): This is the memory on the GPU itself. The entire model and its KV Cache must fit into VRAM. If insufficient VRAM is allocated, the model will fail to load. For example, a 7B parameter model in FP16 requires approximately 14 GB of VRAM.
System RAM: Used by the operating system and for pre-loading operations before the model is transferred to the GPU. Allocate a healthy amount of system RAM (e.g., 16 GB or more) to prevent unexpected pod crashes and ensure stability during startup.
 .. Configure the number of replicas and ensure a GPU is requested in the resource configuration.
 .. OpenShift AI will then automatically provision a pod using the vLLM runtime, pull the model from object storage into the pod’s /mnt/models directory, and start the vLLM server.


[]
====

*Choosing the Right Compute Target (GPU vs. CPU)*

Your first decision is where the model will run. While vLLM can operate in both environments, they serve very different purposes.

[cols="1,2,2",options="header"]
|===
| Feature
| GPU-Accelerated vLLM (Production Standard)
| CPU-Only vLLM (Niche Use Cases)

| **Hardware Requirement**
| (NVIDIA) GPU is **mandatory**.
| Runs on standard x86 CPUs.

| **Performance**
| High-throughput and low-latency. Optimized with custom CUDA kernels for parallel processing.
| Significantly slower. Suitable only for development, testing, or low-load applications with small models.

| **Key Limitation**
| Requires availability of expensive GPU resources and sufficient VRAM.
| Limited data type support. It primarily works with `FP32` and `BF16`, and will automatically convert `FP16` models, which can affect performance and accuracy.
|===

[IMPORTANT]
.VRAM vs. System RAM
****
* **GPU VRAM (Video RAM):** This is the memory on the GPU itself. The *entire model* and its KV Cache must fit into VRAM. If you do not allocate enough VRAM, the model will fail to load. Check the size of your model (e.g., a 7B parameter model in FP16 requires ~14 GB of VRAM) and allocate accordingly.

* **System RAM:** This is the standard server memory. While the model runs on the GPU, system RAM is still used by the operating system and for pre-loading operations before the model is transferred to the GPU. Always allocate a healthy amount of system RAM (e.g., 16 GB or more) to prevent unexpected pod crashes.
****

====


== Interacting with the Deployed Model:

A key advantage of the vLLM runtime is its built-in, **OpenAI-compatible API**. This design choice dramatically simplifies integration, as any application or tool built to communicate with OpenAI's API can be pointed to your self-hosted vLLM endpoint with minimal changes.

=== Interactive Testing with the Swagger UI
The easiest way to test your deployed model is through the built-in Swagger UI, which provides interactive API documentation. You can see all available endpoints, view their parameters, and even send test requests directly from your browser.

* **How to Access:** Simply take the inference endpoint URL provided by the OpenShift AI dashboard and append `/docs` to it.
* **URL:** `https://<your-model-endpoint-url>/docs`

=== Programmatic Integration
For application development, you can use standard HTTP clients or libraries like `openai-python`. The vLLM server implements the core OpenAI endpoints:

* `/v1/chat/completions`: For interacting with conversational or instruction-tuned models.
* `/v1/completions`: For legacy text-completion models.
* `/v1/embeddings`: For generating vector embeddings from text.

.Example: `curl` Request
[source,bash]
----
# 1. Set your model's endpoint URL and name
export ENDPOINT_URL="https://<your-model-endpoint-url>/v1/chat/completions"
export MODEL_NAME="name-of-your-deployed-model" # e.g., "Meta-Llama-3-8B-Instruct"

# 2. Send the POST request
curl -s "$ENDPOINT_URL" \
-H "Content-Type: application/json" \
-d @- << EOF
{
  "model": "$MODEL_NAME",
  "messages": [{"role": "user", "content": "Explain the role of VRAM in LLM inference."}],
  "temperature": 0.7
}
EOF
----

[NOTE]
For more advanced use cases, such as integrating vLLM with frameworks like LangChain, you can find  example notebooks in this courses Github Apps repository and in OpenShift AI documentation. The key is to configure your client to use your model's inference endpoint as the `base_url`.  

