= Red Hat AI Platform Solutions

Red Hat AI models

 * Validated Models - Tested with vLLM & Red Hat Platforms
 * Imdemnified models - Granite base model when used on Red Hat Platforms
 * Supported models - when used on Red Hat Platforms
 * Unsupported models - Any Third party model

To simplify deployment further, Red Hat AI Inference Server includes access to a curated repository of
popular LLMs (such as Llama, Mistral, or Granite families), conveniently hosted on the Red Hat AI page
on Hugging Face.
These aren't just standard models; they have been optimized using the integrated compression
techniques—leveraging Neural Magic's expertise within Red Hat—specifically for high-performance
execution on the vLLM engine, often distributed using efficient formats like safetensors. This means
you get readily deployable, efficient models out-of-the-box, drastically reducing the time and
effort needed to get your AI applications into production and delivering value faster.

We are going to focus on Validated Models located at Red Hat AI on Huggingface.

arcade around select models to showcase the information and the details that made these models.


== AI Specialists: Choosing the Right Model for the Job

    *   **Capability Spectrum:** Discuss the different "skills" of LLMs.
        *   **Text Generation:** Explain models like **Mistral-Small-24B-Instruct-2501-FP8-dynamic** as conversational wizards designed for text-in, text-out tasks. This is your dedicated essay writer or chatbot.
        *   **Multimodal Champions (Image-Text-to-Text):** Introduce models like **Qwen2.5-VL-3B-Instruct-quantized.w8a8** and **gemma-3-4b-it-quantized.w4a16**. Explain that these models can "see" images and "read" text inputs, then generate text outputs. They are perfect for tasks where the AI needs to understand both words and pictures, like interpreting charts or answering questions about documents. This is your AI detective who can analyze both visual evidence and written reports.
    *   **Scale and Specialization:** Briefly discuss parameter counts and how they relate to model size and potential capability.
        *   **Mistral-Small-24B-Instruct-2501-FP8-dynamic:** A substantial brain at **23.6 billion parameters**, quantized for efficiency. The robust all-rounder for textual tasks.
        *   **Qwen2.5-VL-3B-Instruct-quantized.w8a8:** Nimble with **4.07 billion parameters**, a well-trained specialist in multimodal understanding.
        *   **gemma-3-4b-it-quantized.w4a16:** Our project's most compact model at **1.61 billion parameters**, proving "sometimes good things come in small packages" [Previous Response].
    *   **Model Card Deep Dive:** Suggest how the narration can walk through a "Model Card" (e.g.,) as a professional resume for an LLM, highlighting key sections like architecture, optimizations, capabilities, and performance [Previous Response].

5.  **"From Hub to Hosted: Deploying Your LLM Champion"**
    *   **Practical Deployment:** Briefly touch upon the various ways Red Hat AI models can be deployed.
        *   **Direct vLLM:** Show the simple Python examples for deploying with vLLM.
        *   **Red Hat Ecosystem:** Narrate how these models integrate with Red Hat AI Inference Server, Red Hat Enterprise Linux AI (using `ilab`), and Red Hat Openshift AI (using KServe). This demonstrates the "Red Hat AI platform" for deploying generative AI models. This is like having a fully equipped "AI garage" with specialized tools for every kind of vehicle.
    *   **Validation for Peace of Mind:** Reiterate that models are "validated by Red Hat AI", offering comprehensive benchmarking and accuracy evaluations, which means fewer headaches during deployment [Previous Response].

A comprehensive, insightful, and engaging understanding of Red Hat AI's offerings and their relevance to efficient LLM deployment. Good luck with the course!