= Integrating vLLM with OpenShift AI: The Serving Runtime

While you can run vLLM as a standalone server, its true power in an enterprise setting is realized when managed by a robust platform like **Red Hat OpenShift AI (RHOAI)**. To do this, we need to teach OpenShift AI *how* to run vLLM. This is accomplished by defining a **Serving Runtime**.

==== What is a Serving Runtime?

In the context of OpenShift AI and its underlying model-serving framework (KServe), a Serving Runtime is a **reusable template or blueprint** that defines a specific environment for serving a model.

Think of it as a standardized recipe for deploying a model server. Instead of configuring a container image, startup commands, and resource requests for every single model, you define them once in a runtime. When you deploy a model, you simply tell OpenShift AI: "Use this runtime."

This abstraction is a core MLOps principle. It separates the *model artifact* (the "what") from the *serving environment* (the "how"), leading to standardized, repeatable, and maintainable deployments.

A Serving Runtime specifies critical information, such as:

 * The **container image** that contains the model server (e.g., the official vLLM image).
 * The **command and arguments** needed to start the server and load a model.
 * **Hardware requirements**, especially the need for specific accelerators like NVIDIA GPUs.
 * **Network ports** and paths for inference requests and metrics collection (Prometheus).
 * **Supported model formats** that the runtime can handle.

==== The vLLM Serving Runtime for Red Hat OpenShift AI

The OpenShift AI single-model serving stack can use vLLM as a high-performance alternative to other runtimes like Caikit+TGIS. By creating a vLLM `ServingRuntime`, you make it a first-class citizen in the RHOAI model deployment workflow.

[IMPORTANT]
====
*GPU Requirement*

The standard vLLM runtime is built on CUDA kernels for maximum performance. Therefore, a **GPU is required** on the OpenShift worker nodes where the model will be deployed. An experimental CPU version exists with limitations, but for production use cases targeted by this course, we will focus on GPU-based serving.
====

==== Anatomy of the `vllm-runtime.yaml`

To register vLLM with OpenShift AI, an administrator applies a YAML file defining the `ServingRuntime` resource. Let's break down an example to understand its key components.

.vLLM ServingRuntime for NVIDIA GPUs
[%collapsible]
====
[source,yaml,linenums]
----
# Filename: vllm-servingruntime.yaml
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
 name: vllm-cuda-runtime-example
 annotations:
   openshift.io/display-name: vLLM (NVIDIA GPU)
   opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
 labels:
   opendatahub.io/dashboard: 'true'
spec:
 supportedModelFormats:
   - name: vLLM
     autoSelect: true
 containers:
   - name: kserve-container
     image: quay.io/modh/vllm:rhoai-2.20-cuda
     command: ["python", "-m", "vllm.entrypoints.openai.api_server"]
     args:
       - "--port=8080"
       - "--model=/mnt/models"
       - "--served-model-name={{.Name}}"
     env:
       - name: HF_HOME
         value: /tmp/hf_home
     ports:
       - containerPort: 8080
         protocol: TCP
     resources:
       requests:
         nvidia.com/gpu: '1'
       limits:
         nvidia.com/gpu: '1'
----
====

.Key Fields Explained
[cols="1,3"]
|===
|Field (Line #) | Description

|`metadata.name` (4)
|A unique name for the runtime within the namespace.

|`metadata.annotations` (5-7)
|Crucial for UI integration. `openshift.io/display-name` sets the friendly name shown in the OpenShift AI dashboard. `opendatahub.io/recommended-accelerators` signals to the platform that this runtime requires an `nvidia.com/gpu`.

|`spec.supportedModelFormats` (11-13)
|Declares which model formats this runtime supports. `autoSelect: true` allows OpenShift AI to automatically pick this runtime for compatible models.

|`spec.containers.image` (15)
|The specific container image to run. Here, it points to an official image for vLLM on RHOAI. For AMD GPUs, this would be a ROCm image.

|`spec.containers.args` (17-19)
|The command-line arguments passed to the vLLM server at startup.
*`--port=8080`*: Exposes the server on this port.
*`--model=/mnt/models`*: This is critical. It tells vLLM to load the model from the `/mnt/models` directory, which is the standard location where OpenShift AI's serving platform mounts model data.
*`--served-model-name={{.Name}}`*: A template variable that passes the deployed model's name to the server.

|`spec.containers.resources` (25-29)
|Defines the resource requests and limits. Requesting and limiting `nvidia.com/gpu: '1'` ensures the pod is scheduled on a GPU-equipped node and is allocated one GPU.
|===

==== Installation and Deployment Workflow

Once the `ServingRuntime` is defined, the workflow for a platform engineer or data scientist is straightforward.

.Step 1: Register the Runtime (Admin Task)
An administrator with cluster privileges must first install the necessary OpenShift AI components (like the single-model serving stack). Then, they can add the vLLM runtime:

1.  Navigate to the OpenShift AI Dashboard.
2.  Go to **Settings -> Serving runtimes**.
3.  Click **Add serving runtime** and choose `Single model serving platform`.
4.  Paste the content of the `vllm-servingruntime.yaml` file into the editor and click `Create`.

The "vLLM (NVIDIA GPU)" runtime will now be available for selection when users deploy models.

.Step 2: Deploy a Model (User Task)
With the runtime available, a user can deploy a compatible model:

1.  Ensure your model files (e.g., from Hugging Face) are located in an S3-compatible object storage bucket.
2.  In your OpenShift AI project, create a **Data Connection** that points to this bucket.
3.  From the project dashboard, click **Deploy model**.
4.  Fill in the model name, select the `vLLM (NVIDIA GPU)` serving runtime, and point to the data connection and specific model path.
5.  Configure the number of replicas and ensure a **GPU is requested** in the resource configuration.

OpenShift AI will then automatically provision a pod using the vLLM runtime, pull the model from object storage into the pod's `/mnt/models` directory, and start the vLLM server.

.Step 3: Interact with the Endpoint
Once the model is running, the dashboard provides an inference endpoint URL. Because the runtime exposes an **OpenAI-compatible API**, you can interact with it using standard tools.

[TIP]
====
vLLM provides a full Swagger UI for API exploration and direct interaction. Simply add `/docs` to the end of your inference endpoint URL (e.g., `https://your-model-endpoint/docs`) to access it. This is an excellent way to test the model without writing any code.
====

A simple `curl` request would look like this:

[source,bash]
----
# Replace with your actual endpoint and model name
export ENDPOINT_URL="https://your-model-endpoint/v1/chat/completions"
export MODEL_NAME="meta-llama/Meta-Llama-3-8B-Instruct"

curl -X POST "$ENDPOINT_URL" \
-H "Content-Type: application/json" \
-d '{
  "model": "'"$MODEL_NAME"'",
  "messages": [{"role": "user", "content": "What are the benefits of using a serving runtime?"}]
}'
----