= GPU VRAM Blueprint
:toc:
:sectnums:
:icons: font

_Estimating GPU Memory for Large Language Model Inference_

This document serves as a quick reference guide for estimating the VRAM (GPU memory) required to run Large Language Models for inference.

== How Much VRAM for Model Weights?

The first step in planning your hardware is calculating the VRAM needed to simply store the model's weights. The required memory is determined by the model's size (number of parameters) and its numerical precision. Using lower precision, like INT8 or INT4, can drastically reduce the memory footprint.

.Estimated VRAM for Model Weights by Precision
[options="header"]
|===
| Model Size (Parameters) | FP16 / BF16 VRAM | INT8 VRAM | INT4 VRAM

| *1 Billion*
| ~2 GB
| ~1 GB
| ~0.5 GB

| *3 Billion*
| ~6 GB
| ~3 GB
| ~1.5 GB

| *7 Billion*
| ~14 GB
| ~7 GB
| ~3.5 GB

| *13 Billion*
| ~26 GB
| ~13 GB
| ~6.5 GB

| *30 Billion*
| ~60 GB
| ~30 GB
| ~15 GB

| *65 Billion*
| ~130 GB
| ~65 GB
| ~32.5 GB
|===

== Beyond Weights: The Real VRAM Consumers

Model weights are just the baseline. Real-world inference requires budgeting for several other critical components that consume significant VRAM.

=== KV Cache

Memory Cost:: *+100% or More*
Description:: This is memory used to store the attention keys and values for the tokens in the input sequence. The KV Cache is the *largest source of overhead* and its size grows linearly with the batch size and context length. For applications with long context windows or high batch throughput, the KV Cache can easily consume more memory than the model weights themselves.

=== CUDA & System Overhead

Memory Cost:: *+10-20%*
Description:: This overhead includes the memory consumed by the CUDA kernels that perform the computations, the framework libraries (like PyTorch and vLLM), and various system buffers. This is a fixed cost you pay just for having the inference environment loaded on the GPU.

=== Model Activations

Memory Cost:: *+1-2%*
Description:: These are the intermediate calculations that are stored during the model's forward pass. While the memory impact is much smaller than the KV Cache, it is a non-zero factor that contributes to the total VRAM load.

== The Real-World VRAM Equation

A more realistic way to think about your total memory requirement is with the following formula:

[IMPORTANT]
.Key Takeaway
====
A model's sticker price in VRAM is not the final cost. A 7B parameter model might list a ~14 GB requirement for its FP16 weights, but with a large context window for the KV Cache, the *actual VRAM requirement can easily exceed 24 GB*. Always profile your specific use case; don't just calculate based on weights.

A simple mental model for total VRAM is:

[source,text]
----
Total VRAM Needed â‰ˆ (Model Weights VRAM) + (KV Cache VRAM) + (System Overhead)
----
====

---
_vLLM Course Content | Built for Delivery Engineers & Consultants_ | with the help of Gemini