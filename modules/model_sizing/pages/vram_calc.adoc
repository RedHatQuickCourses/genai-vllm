= GPU VRAM Blueprint


== Estimating GPU Memory for Large Language Model Inference_

This document serves as a quick reference guide for estimating the VRAM (GPU memory) required to run Large Language Models for inference.

== How Much VRAM for Model Weights?

The first step in planning your hardware is calculating the VRAM needed to simply store the model's weights. The required memory is determined by the model's size (number of parameters) and its numerical precision. Using lower precision, like INT8 or INT4, can drastically reduce the memory footprint.

.Estimated VRAM for Model Weights by Precision
[options="header"]
|===
| Model Size (Parameters) | FP16 / BF16 VRAM | INT8 VRAM | INT4 VRAM

| *1 Billion*
| ~2 GB
| ~1 GB
| ~0.5 GB

| *3 Billion*
| ~6 GB
| ~3 GB
| ~1.5 GB

| *7 Billion*
| ~14 GB
| ~7 GB
| ~3.5 GB

| *13 Billion*
| ~26 GB
| ~13 GB
| ~6.5 GB

| *30 Billion*
| ~60 GB
| ~30 GB
| ~15 GB

| *65 Billion*
| ~130 GB
| ~65 GB
| ~32.5 GB
|===

== Beyond Weights: The Real VRAM Consumers

Model weights are just the baseline. Real-world inference requires budgeting for several other critical components that consume significant VRAM.

=== KV Cache

Memory Cost:: *+100% or More*
Description:: This is memory used to store the attention keys and values for the tokens in the input sequence. The KV Cache is the *largest source of overhead* and its size grows linearly with the batch size and context length. For applications with long context windows or high batch throughput, the KV Cache can easily consume more memory than the model weights themselves.

=== CUDA & System Overhead

Memory Cost:: *+10-20%*
Description:: This overhead includes the memory consumed by the CUDA kernels that perform the computations, the framework libraries (like PyTorch and vLLM), and various system buffers. This is a fixed cost you pay just for having the inference environment loaded on the GPU.

=== Model Activations

Memory Cost:: *+1-2%*
Description:: These are the intermediate calculations that are stored during the model's forward pass. While the memory impact is much smaller than the KV Cache, it is a non-zero factor that contributes to the total VRAM load.

== The Real-World VRAM Equation

A more realistic way to think about your total memory requirement is with the following formula:

[IMPORTANT]
.Key Takeaway
====
A model's sticker price in VRAM is not the final cost. A 7B parameter model might list a ~14 GB requirement for its FP16 weights, but with a large context window for the KV Cache, the *actual VRAM requirement can easily exceed 24 GB*. Always profile your specific use case; don't just calculate based on weights.

A simple mental model for total VRAM is:

[source,text]
----
Total VRAM Needed ≈ (Model Weights VRAM) + (KV Cache VRAM) + (System Overhead)
----
====


== Understanding GPU Memory Allocation During Inference

When running an AI model for inference, GPU memory is consumed by several components in addition to the model's weights. Understanding these is crucial for accurately provisioning hardware and optimizing performance.

=== Model Weights

This is the most straightforward component. It's the memory required to load the actual *parameters* of the neural network. The size is determined by the number of parameters and the precision used (e.g., a 7-billion-parameter model at half-precision (FP16) requires approximately 14 GB).

=== Key-Value (KV) Cache

For autoregressive models like transformers (the basis for most LLMs), the *Key-Value (KV) Cache* is a major memory consumer. To generate a new token, the model must consider the tokens that came before it. Instead of recomputing this context for every new token, the model stores intermediate calculations—the "keys" and "values" from the attention mechanism—in the KV cache.

* *Why it's important:* It dramatically speeds up token generation after the initial prompt is processed.
* *Memory Impact:* The size of the KV cache grows with the *sequence length* (prompt + generated tokens) and the *batch size* (number of simultaneous requests). For long contexts or high-volume services, the KV cache can often consume more memory than the model weights themselves.

=== Activations

Activations are the *intermediate outputs* of a model's layers as an input signal passes through the network. During inference, the GPU must hold the activations for the current input being processed. While these are transient and replaced with each new input, they still occupy a significant chunk of memory, especially for models with many layers or large hidden states.

=== Framework and Execution Overhead

Additional memory is used by the underlying software and hardware systems to manage the inference process. This includes:

* *CUDA Context:* The NVIDIA driver and libraries require a certain amount of VRAM to initialize and manage the GPU.
* *Inference Engine Buffers:* Frameworks like vLLM, TensorRT, or PyTorch allocate buffers to hold the input data, the final output, and other temporary tensors needed for computation.
* *Memory Fragmentation:* Over time, memory can become fragmented, leaving small, unusable gaps between allocated blocks, which contributes to overall memory usage.