= Estimating GPU VRAM 


== Cost-Aware Model and Hardware Selection

In the previous sections, we focused on the vLLM engine and its deployment on OpenShift AI. Now, we will shift our focus to the economics of AI infrastructure. This module will equip you with the knowledge to make cost-effective decisions by aligning LLM selection, compression techniques, and hardware choices.

=== The VRAM Blueprint: Estimating True Memory Costs

The first step in cost-effective deployment is accurately estimating your GPU memory (VRAM) requirements. Under-provisioning leads to out-of-memory errors and service downtime, while over-provisioning leads to idle, expensive hardware and wasted budget. This section provides a blueprint for calculating the *true* VRAM footprint of a model in production.

=== The Baseline Cost: Model Weights

The most straightforward VRAM cost is the space needed to load the model's parameters (weights). This is a function of two variables: the size of the model (in billions of parameters) and the numerical precision used to store each parameter.

Using a lower-precision format via **quantization** is the most direct way to reduce this baseline memory cost.

.Estimated VRAM for Model Weights by Precision
[options="header"]
|===
| Model Size (Parameters) | FP16 / BF16 (16-bit) | INT8 (8-bit) | INT4 (4-bit)

| *1 Billion* | ~2 GB | ~1 GB | ~0.5 GB
| *3 Billion* | ~6 GB | ~3 GB | ~1.5 GB
| *7 Billion* | ~14 GB | ~7 GB | ~3.5 GB
| *13 Billion* | ~26 GB | ~13 GB | ~6.5 GB
| *30 Billion* | ~60 GB | ~30 GB | ~15 GB
| *70 Billion* | ~140 GB | ~70 GB | ~35 GB
|===

[NOTE]
The calculation is simple: VRAM (GB) ≈ (Number of Parameters in Billions) x (Bits per Parameter / 8). A 7B model at FP16 is `7 * (16 / 8) = 14 GB`.

=== The Hidden Costs: Beyond the Weights

A common mistake is to select a GPU based only on the model weight VRAM. In a live production environment, several other components consume significant memory, and they must be factored into your budget.

.VRAM Usage Components
image::vram_components.png[A conceptual stacked bar chart showing that total VRAM is composed of a large block for Model Weights, an equally large or larger block for the KV Cache, and a smaller block for System Overhead.]

=== 1. The KV Cache (The Memory Hog)
The Key-Value (KV) Cache stores attention data for the sequence being processed. For modern LLMs, this is often the **largest and most volatile consumer of VRAM**. Its size is not fixed; it grows dynamically based on your workload.

* **Key Drivers:** The size of the KV Cache is directly proportional to:
    * **Batch Size:** The number of requests you process concurrently.
    * **Context Length:** The number of tokens (input + output) in each request.
* **Impact:** For applications with long context windows (e.g., document summarization) or high batch sizes, the KV Cache can easily consume **more VRAM than the model weights themselves**. A 14 GB model might require another 16 GB or more for its KV Cache under heavy load.

[TIP]
This is precisely the problem vLLM was designed to solve. Its core innovation, **PagedAttention**, drastically reduces the memory wasted by the KV Cache, allowing you to handle larger batches and longer contexts on the same GPU, directly improving your cost-performance ratio.

=== 2. CUDA & System Overhead
This is the fixed cost of doing business on a GPU. It includes memory consumed by the NVIDIA CUDA kernels, the core PyTorch and vLLM libraries, and various system buffers required to manage the computation.
* **Estimated Cost:** Budget an additional *10-20%* of the model's weight VRAM for this overhead.

=== 3. Model Activations
These are the intermediate values calculated during the model's forward pass. While their memory impact is far smaller than the KV Cache, they are a non-zero factor in the total VRAM load.

=== The Real-World VRAM Equation

Therefore, a practical formula for estimating your total memory requirement looks like this:

[source,text]
----
Total VRAM Needed ≈ (VRAM for Model Weights) + (VRAM for Max KV Cache) + (VRAM for System Overhead)
----

[IMPORTANT]
.Don't Be Fooled by the "Sticker Price"
====
A model's advertised size is not its final cost in production. A 13B parameter model might list a ~26 GB requirement for its FP16 weights, suggesting it could fit on a 32 GB GPU. However, with a large batch size and long context window for the KV Cache, the *actual VRAM requirement can easily exceed 40 GB*.

**Golden Rule:** Always profile your specific use case with realistic batch sizes and context lengths. Never select hardware based solely on the VRAM needed for model weights.
====

== Connecting VRAM to Infrastructure Cost

Understanding the VRAM requirements allows you to estimate the annual infrastructure cost. Below are estimates for common GPU instances based on a 1-year commitment.

.Annual Cloud Cost Estimates
[options="header"]
|===
| VRAM per GPUs | Example AWS Instance | Estimated Annual Cost | Notes
| 24 GB        | `g6.4xlarge`         | *$7,000 - $8,000* | *Recommended starting point for PoCs*
| 48 GB        | `g6e.2xlarge`        | $12,000 - $14,000      | Cost doubles from the 24GB tier
| 192 GB (4x48)     | `g6e.12xlarge`       | $55,000 - $65,000      | For multi-model serving or very large models
| 640 GB (8x80)     | `p5.48xlarge`        | $240,000+              | Enterprise scale (based on monthly cost)
|===

== Our Recommended Project Strategy

For a typical customer Proof-of-Concept (PoC) with a limited budget, follow this strategic workflow.

.Target the Sweet Spot
Start by targeting the **24 GB VRAM** infrastructure (`g6.4xlarge`). This class of GPU offers the best performance-per-dollar and aligns with customer budgets for initial projects.

.Search with Intent
Filter your model search to those that provide **quantized versions**. A quantized 13B model can often outperform a non-quantized 7B model while fitting in the same 24GB memory budget.

.Validate and Iterate
Deploy your chosen model and benchmark its performance *and* real-world VRAM consumption. Be prepared to test different models to find the optimal balance of speed, accuracy, and cost for the customer's specific use case.

== Red Hat Sizing Guide

Intended to help provide *a model for estimations for sizing clusters for OpenShift AI* based on a few questions about the customers intended usage.

Internal Only - http://red.ht/rhoai-sizing-guide[OpenShift AI Cluster sizing sheet]

slack channel #help-rhoai-sizing-guide

