= Lab Environment Setup

Before proceeding with vLLM model deployment, it is necessary to prepare the lab environment. This section provides an overview of the setup for a self-managed Red Hat OpenShift AI instance, including the required dependent operators and the NVIDIA GPU Operator for hardware acceleration.

== Red Hat OpenShift AI Installation

The foundation of our lab is a self-managed instance of Red Hat OpenShift AI. This platform provides the core services for deploying and managing machine learning models. A self-managed installation offers a comprehensive, hands-on understanding of the platform's architecture.

=== Required Operators

OpenShift Operators are a fundamental component for managing the lifecycle of applications within the cluster. The installation of the primary **Red Hat OpenShift AI Operator** orchestrates the deployment of the platform.

For the single-model serving capabilities used in this course, the following operators must also be present in the cluster. While the OpenShift AI operator often manages these dependencies, it is important to verify their installation.

* **OpenShift Service Mesh Operator**: Provides essential networking capabilities for secure and reliable communication between model serving microservices.
* **OpenShift Serverless Operator**: Enables efficient resource utilization by allowing serving pods to scale based on demand, including scaling down to zero.

These operators are available for installation from the **OperatorHub** within the OpenShift web console.

== NVIDIA GPU Operator for Hardware Acceleration

To achieve high-performance model inference with vLLM, access to NVIDIA GPUs is required. The **NVIDIA GPU Operator** automates the complex process of configuring GPU-enabled nodes in the OpenShift cluster. It manages all necessary NVIDIA software components, including drivers and the container runtime, simplifying the use of GPUs for AI workloads.

=== NVIDIA Operator Installation

The installation process for the NVIDIA GPU Operator can be summarized in these steps:

.   **Install from OperatorHub**: Locate and install the Node Feature Discovery operator from the OpenShift OperatorHub.  

.   **Create a `ClusterPolicy`**: After the operator is installed, a `ClusterPolicy` custom resource (CR) must be created. This CR instructs the operator to begin the configuration of the GPU worker nodes. The default configuration provided by the operator is sufficient for most use cases.

With these foundational components configured, the OpenShift AI environment is prepared for deploying models with the vLLM serving runtime.

'''

The next step is to create a *Data Connection* in our Data Science Project. Before we can create our Data Connection, we will set up MinIO as our S3 compatible storage for this Lab.

Continue to the next section to deploy and configure MinIO.