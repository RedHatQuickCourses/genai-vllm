= NVIDIA GPU Architectures
:toc: left
:toclevels: 2
:sectnums:
:icons: font

_Comparing key advancements in GPU technology from Ampere to Blackwell, driving AI and High-Performance Computing (HPC) forward._

== GPU Architecture Showdown

This section details the key features and improvements across NVIDIA's major datacenter-focused GPU architectures.

=== Ampere (e.g., A100)

*Architecture*:: NVIDIA Ampere
*Target Applications*:: AI Training & Inference, HPC
*Key Innovations*:: 3rd Gen Tensor Cores, Multi-Instance GPU (MIG), Structural Sparsity
*Memory Bandwidth*:: ~1.5-2.0 TB/s
*NVLink*:: NVLink 3 (Up to 600 GB/s per GPU)
*Precision Support*:: FP32, FP16, BF16, TF32
*Transistor Count*:: ~54 Billion
*Availability*:: Since 2020

=== Hopper (e.g., H100)

*Architecture*:: NVIDIA Hopper
*Target Applications*:: Large-Scale AI & HPC, Transformer Models
*Key Innovations*:: 4th Gen Tensor Cores, Transformer Engine, FP8 Precision
*Memory Bandwidth*:: ~3.0-4.0 TB/s
*NVLink*:: NVLink 4 (Up to 900 GB/s per GPU)
*Precision Support*:: FP32, FP16, FP8
*Transistor Count*:: ~80 Billion
*Availability*:: Since 2022

=== Ada Lovelace (e.g., A6000, L40)

*Architecture*:: NVIDIA Ada Lovelace
*Target Applications*:: Professional Graphics, AI Inference, Content Creation
*Key Innovations*:: 3rd Gen RT Cores, 4th Gen Tensor Cores, DLSS 3, AV1 Encode/Decode
*Memory Bandwidth*:: Varies (e.g., L40 has ~864 GB/s)
*Precision Support*:: FP32, FP16 (with INT8/FP8 on select models)
*Transistor Count*:: ~76.3 Billion (AD102 Chip)
*Availability*:: Since 2022

[NOTE]
====
The Ada Lovelace architecture bridges professional graphics with strong AI inference capabilities. Hopper and Blackwell are more sharply focused on large-scale, distributed AI and HPC workloads in the datacenter.
====

=== Blackwell (e.g., B100, B200)

*Architecture*:: NVIDIA Blackwell
*Target Applications*:: Trillion-Parameter LLMs, Generative AI, Data Analytics
*Key Innovations*:: 2nd Gen Transformer Engine, FP4 & FP6 Precision, Decompression Engine, Secure AI
*Memory Bandwidth*:: ~8-10 TB/s
*NVLink*:: NVLink 5 (Up to 1.8 TB/s per GPU)
*Precision Support*:: FP32, FP16, FP8, FP6, FP4
*Transistor Count*:: ~208 Billion (per B200 die)
*Availability*:: Announced 2024

== Growth in Power: Transistor Counts

The massive increase in transistor density is a primary driver of the performance leap between generations.

.Transistor Count Comparison
[options="header"]
|===
| Architecture | Example GPU | Transistor Count (Billions)
| Ampere | A100 | 54
| Hopper | H100 | 80
| Blackwell | B200 | 208
|===

== NVIDIA's Relentless AI Focus

The evolution across these architectures demonstrates a clear and intense focus on accelerating AI workloads through specialized hardware and software.

* The introduction of the *Transformer Engine* in Hopper, further enhanced in Blackwell, to dramatically speed up AI models.
* Continuous improvements in *Tensor Core technology* across generations for superior matrix multiplication performance.
* Support for new, *lower-precision formats like FP8, FP6, and FP4*, enabling faster computations and reduced memory footprints for AI.
* Massive increases in *memory bandwidth and NVLink interconnect speed* to feed the increasingly powerful compute units and enable larger, more complex models.

---
_Data based on publicly available information._
