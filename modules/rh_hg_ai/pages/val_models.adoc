= Validated Models and Quantization Strategies

The open-source AI landscape, particularly on platforms like Hugging Face, can feel like the "Wild West"—a vast space with thousands of models, making it difficult to know which ones are performant, accurate, or ready for enterprise use.

This module introduces the **Red Hat AI Validated Model Repository**, a curated ecosystem designed to solve this problem. You will learn how to leverage this resource to de-risk your projects, accelerate deployment, and make informed decisions based on transparent, rigorous testing.

We will specifically focus on understanding the significance of model quantization and how Red Hat AI offers various quantized formats to optimize LLM inference for enterprise environments.

---

=== The Red Hat AI Validated Model Program

The Red Hat AI repository on Hugging Face is an open-source initiative born from a deep collaboration between IBM and Red Hat. Its mission is to bridge the gap between cutting-edge research and robust production deployments by providing open, accessible, and community-driven AI solutions.

By sharing models, research, and validated configurations, the program empowers you to deploy high-performance AI at scale with confidence.

==== Why Use a Validated Model?

A "validated model" from the Red Hat AI repository is more than just a pre-trained LLM. It's a model that has been meticulously assessed through comprehensive testing, providing critical insights into its real-world behavior.

[IMPORTANT]
.Key Benefits of Using a Validated Model
****
* **Reduces Guesswork:** Provides reliable performance expectations and recommended deployment settings, eliminating the need for extensive trial-and-error.
* **Ensures Enterprise Readiness:** Validates that models perform well under load using enterprise-grade tools like vLLM.
* **Optimizes for Cost:** Offers clear guidance on pairing models with the right hardware, maximizing inference efficiency and lowering Total Cost of Ownership (TCO).
* **Provides Flexibility:** Offers a range of popular models in various optimized formats to meet diverse project requirements.
****

==== The Validation Process

Red Hat's validation process uses industry-standard open-source tooling to ensure the results are transparent and reproducible:

* **`GuideLLM`:** A powerful benchmarking tool used for capacity planning. It simulates real-world traffic to assess a model's throughput, latency, and resource utilization on various hardware setups.
* **`Language Model Evaluation Harness` (LM Eval Harness):** The industry standard for accuracy evaluation. It measures a model's reasoning and generalization capabilities across a wide range of academic benchmarks.
* **`vLLM`:** The inference engine used during performance validation. This ensures that the published benchmarks reflect the performance you can expect when deploying with the tools taught in this course.

---

=== A Deep Dive into Quantization Strategies

Model quantization is a critical optimization technique that plays a central role in Red Hat AI’s validated model collection. It involves reducing the numerical precision of model weights and/or activations, typically from higher-precision floating-point formats (e.g., FP32 or FP16) to lower-bit representations (e.g., FP8, INT8, INT4).

==== Understanding the Naming Convention

When Browse the repository, you will see suffixes like `.w4a16` or `.FP8-dynamic`. This is a standardized shorthand that tells you the precision of the model's **weights (w)** and **activations (a)**.

* **`w4a16`**: The model's **w**eights are quantized to 4-bit integers, and the **a**ctivations (the in-flight calculations) are processed at 16-bit precision (FP16/BF16). This is a very common "weight-only" quantization scheme.
* **`w8a8`**: Both weights and activations are processed at 8-bit integer precision.
* **`FP8` or `FP8-dynamic`**: The model leverages the 8-bit floating-point format, often with dynamic scaling to maintain accuracy.

==== The Performance vs. Accuracy Trade-Off

No single quantization level is best for every situation. The key is to balance performance, accuracy, and cost for your specific use case.

.Quantization Strategy Comparison
[options="header"]
|===
| Precision | Best For | Hardware Alignment | Accuracy Risk

| **FP16 / BF16**
| Highest accuracy, baseline performance.
| All modern GPUs.
| **Lowest (None)**

| **FP8**
| **Excellent balance of speed and accuracy.**
| Natively accelerated on **Hopper (H100), Ada (L4/L40S), and Blackwell (B100)** GPUs.
| **Low.** Generally preserves accuracy better than integer formats.

| **INT8**
| Good speed and memory savings on a wide range of hardware.
| Widely supported, but most performant on GPUs with INT8 Tensor Cores.
| **Medium.** Can cause accuracy degradation in some models; requires testing.

| **INT4**
| **Maximum memory reduction.** Fitting large models on smaller GPUs.
| All modern GPUs, but performance benefits depend on efficient software implementation (e.g., via vLLM).
| **Highest.** Requires advanced quantization techniques (like AWQ/GPTQ) to mitigate accuracy loss.
|===

[TIP]
As discussed in the previous module, your quantization choice should be **hardware-aware**. If your target is an H100 or L40S GPU, an `FP8` quantized model is the most strategic choice as you are leveraging a feature the hardware was specifically designed to accelerate.


== v1.0 Collection of Red Hat AI Validated Models

This initial collection features leading third-party generative AI models rigorously validated for efficient use across the Red Hat AI Product Portfolio. Each model listed below has associated configurations for various quantization levels (e.g., `w4a16` for 4-bit weights and 16-bit activations, `w8a8` for 8-bit weights and 8-bit activations, or `FP8-dynamic` for dynamic FP8 quantization).

* Gemma-3 Quantized
* Whisper Quantized (Note: Whisper is typically an ASR model, but its text-generation component is relevant)
* Llama 4 Quantized
* Qwen3 Quantized
* Mistral Small-3.1 Instruct Quantized
* Phi-4 Quantized
* Llama 3.3 70B Instruct Quantized
* Qwen 2.5 Quantized
* Granite Quantized

[NOTE]
The specific quantization scheme (e.g., `w4a16`, `w8a8`, `FP8-dynamic`) for each model is typically indicated in its full name or metadata within the Red Hat AI Hugging Face repository. These notations signify the precision used for weights (w) and activations (a), or the dynamic FP8 method.

==== Availability and Selection Criteria

While Red Hat AI focuses on validated models, other models may also be available in the repository but have not undergone the full validation process. For practical deployment in this course, we will focus on specific pre-selected models from the validated collection.

---

==== Navigating and Selecting a Validated Model


This final lesson guides you through the model collections and the criteria used to select them, empowering you to make smart choices for your own projects.

////

==== Enterprise Selection Criteria

The models in the validated collection are not chosen at random. They are selected based on criteria that reflect real-world enterprise constraints and goals, including:

* **Performance on Cost-Effective Hardware:** Can the model run effectively on an NVIDIA L4 or A10G GPU (24GB VRAM)?
* **Total Cost of Ownership (TCO):** Does the deployment align with a reasonable yearly infrastructure budget (e.g., < $75,000)?
* **Demonstrated Business Impact:** Can the model solve tangible business problems, like improving developer productivity or automating customer support?
////

==== Pre-Selected Models for This Course

For the hands-on labs in this course, we have pre-selected a representative sample from the Red Hat AI validated repository. These models showcase different sizes, architectures, and quantization strategies, giving you a broad range of experience.

* `RedHatAI/Mistral-7B-Instruct-v0.3-quantized.w4a16`
* `RedHatAI/Qwen2-7B-Instruct-quantized.w8a8`
* `RedHatAI/gemma-2-9b-it-quantized.FP8-dynamic`

These models provide a perfect starting point for exploring the trade-offs between a 4-bit weight quantized model, a full 8-bit model, and a modern FP8 model.

#### Next Steps

In the following labs, we will dive deep into these three models. You will deploy each one using vLLM on OpenShift AI, benchmark their performance, and evaluate their outputs, giving you the practical experience needed to select the right model for your next project.