= Intro Quantization Strategies

Quantization Strategies for Cost-Effective AI Deployment

Model selection and deployment on OpenShift AI are heavily influenced by **quantization strategies**, which are central to optimizing LLM inference, especially in enterprise environments where cost-efficiency is paramount.

== What is Quantization and Why is it Important?

**Quantization is a critical optimization technique** that involves **reducing the numerical precision of model weights and/or activations**. Typically, this means converting from higher-precision floating-point formats (like FP32 or FP16) to lower-bit representations (such as FP8, INT8, or INT4).

Its core importance lies in its ability to **fit powerful models onto cost-effective hardware** by significantly reducing memory footprint and potentially increasing inference speed.

== Understanding Quantization Naming Conventions

When navigating the Red Hat AI repository, you will encounter standardized suffixes in model names that indicate their quantization precision:

 *   **`.w4a16`**: This denotes that the model's **w**eights are quantized to 4-bit integers, while the **a**ctivations (in-flight calculations) are processed at 16-bit precision (FP16/BF16). This is a common "weight-only" quantization scheme.
 *   **`.w8a8`**: Both the model's **w**eights and **a**ctivations are processed at 8-bit integer precision.
 *   **`.FP8` or `.FP8-dynamic`**: The model leverages the 8-bit floating-point format, often with dynamic scaling to maintain accuracy.


== Performance vs. Accuracy Trade-Off and Hardware Alignment

There is no single "best" quantization level; the optimal choice depends on balancing performance, accuracy, and cost for a specific use case. A **hardware-aware strategy is essential** to align the model’s quantization with the GPU’s native capabilities. This strategic alignment allows for the efficient use of both **existing hardware or newly acquired hardware** for on-premises deployments, which directly contributes to a lower Total Cost of Ownership.

Here's a breakdown of common quantization strategies and their implications:

.Quantization Strategy Comparison
[options="header"]
|===
| Precision | Best For | Hardware Alignment | Accuracy Risk

| **FP16 / BF16**
| Highest accuracy, baseline performance.
| All modern GPUs.
| **Lowest (None)**

| **FP8**
| **Excellent balance of speed and accuracy.**
| Natively accelerated on **Hopper (H100), Ada (L4/L40S), and Blackwell (B100)** GPUs.
| **Low.** Generally preserves accuracy better than integer formats.

| **INT8**
| Good speed and memory savings on a wide range of hardware.
| Widely supported, but most performant on GPUs with INT8 Tensor Cores.
| **Medium.** Can cause accuracy degradation in some models; requires testing.

| **INT4**
| **Maximum memory reduction.** Fitting large models on smaller GPUs.
| All modern GPUs, but performance benefits depend on efficient software implementation (e.g., via vLLM).
| **Highest.** Requires advanced quantization techniques (like AWQ/GPTQ) to mitigate accuracy loss.
|===


== Total Cost of Ownership (TCO) Considerations

For AI platform delivery engineers, it's crucial to think in terms of TCO. By leveraging the performance and efficiency data provided with validated models, you can **justify hardware selections and model choices to stakeholders**. Framing decisions around "performance-per-dollar" and "Queries Per Dollar" clearly demonstrates the business value of optimized deployments.


