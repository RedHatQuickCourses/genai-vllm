= Red Hat AI: Validated Models and Quantization Strategies

This section introduces the Red Hat AI initiative, highlighting its commitment to open AI and providing access to a curated repository of validated models. We will specifically focus on understanding the significance of model quantization within this context and how Red Hat AI offers various quantized formats to optimize LLM inference for enterprise environments.

== 1. The Red Hat AI Initiative

The Red Hat AI repository on Hugging Face represents a strategic open-source initiative, born from a deep collaboration between IBM and Red Hat's research, engineering, and business units. This collaboration underscores a commitment to making AI more accessible, efficient, and community-driven, bridging the gap from cutting-edge research to robust production deployments.

Red Hat firmly believes in the future of open AI. By sharing its latest models, research, and validated configurations on Hugging Face, Red Hat aims to empower researchers, developers, and organizations to deploy high-performance AI at scale, leveraging the collective power of the open-source community.

== 2. Red Hat AI Validated Models: Ensuring Enterprise Readiness

To facilitate the efficient and predictable deployment of LLMs across Red Hat platforms, Red Hat AI provides access to a specialized repository of third-party models that have undergone rigorous validation.

=== 2.1. Why Validated Models?

These validated models are not merely a collection of pre-trained LLMs. They are meticulously assessed through a series of comprehensive capacity guidance planning scenarios. This validation process provides critical insights, enabling delivery engineers and consultants to make informed decisions regarding:

* **Optimal Model Selection:** Identifying the most suitable model for specific domain use cases.
* **Deployment Settings:** Understanding recommended configurations for optimal performance.
* **Hardware Accelerator Pairing:** Determining the right combination of model and hardware for efficiency.

This rigorous validation translates into several key benefits for enterprises:

* **Confidence and Predictability:** Reduces guesswork and provides reliable performance expectations.
* **Flexibility:** Offers a range of optimized models and configurations to meet diverse requirements.
* **Cost Efficiency:** Guides users toward deployments that maximize inference efficiency across available hardware, ultimately lowering operational costs.

Red Hat intends to continuously update this collection, typically releasing new sets of validated models on a monthly basis, aligning with the cadence of upstream vLLM releases. While Red Hat strives for consistency, it reserves the right to adjust its validation cadence or stop validating specific models if necessary.

=== 2.2. The Validation Process

Red Hat AI's validation process leverages industry-standard open-source tooling to ensure reproducibility and transparent evaluation:

* **GuideLLM:** Used for comprehensive benchmarking and capacity planning, assessing model performance and resource utilization across various hardware setups.
* **Language Model Evaluation Harness (LM Eval Harness):** Employed for accuracy evaluations, measuring the model's generalization capabilities across a wide range of tasks and datasets.
* **vLLM:** Utilized as the inference serving engine for performance validation, ensuring the models run efficiently with vLLM's optimizations.

== 3. Understanding Model Quantization: Why Different Precisions?

Model quantization is a critical optimization technique that plays a central role in Red Hat AI's validated model collection. It involves reducing the numerical precision of model weights and/or activations, typically from higher-precision floating-point formats (e.g., FP32 or FP16) to lower-bit representations (e.g., FP8, INT8, INT4).

=== 3.1. The Need for Various Quantization Options

The availability of models in different quantized formats (FP8, INT8, INT4) is driven by a fundamental trade-off between **inference performance (speed, memory usage)** and **model accuracy**. No single quantization level is universally optimal; the best choice depends on several factors:

* **Hardware Capabilities:**
    * Newer NVIDIA GPU architectures (e.g., Hopper, Ada Lovelace) have dedicated Tensor Cores optimized for FP8 computations, offering significant speedups with minimal accuracy loss for this precision.
    * INT8 is widely supported across a broader range of hardware and often provides a good balance.
    * INT4 can be highly memory-efficient but may require specific hardware support or more complex software techniques (like weight packing) for efficient computation, and comes with the highest risk to accuracy.

* **Application Requirements:**
    * **Latency-Critical Applications:** May prioritize maximum speed (e.g., FP8 on compatible hardware, or INT4 if accuracy degradation is acceptable).
    * **Resource-Constrained Environments:** Focus on minimizing memory footprint (e.g., INT4 for edge deployments or maximizing batch size on a single GPU).
    * **High-Accuracy Demands:** Might prefer higher precisions or careful evaluation of lower-bit options.

* **Model Robustness to Quantization:**
    * Some LLM architectures or specific models are inherently more "robust" to quantization (i.e., they lose less accuracy when quantized) than others.
    * Rigorous testing (like that performed by Red Hat AI) is essential to determine the actual performance-accuracy trade-off for a given model and quantization level.

* **Cost Efficiency:**
    * Lower precision often means less memory bandwidth, less power consumption, and the ability to serve more requests per GPU or run on less expensive hardware. This directly translates to reduced operational costs for AI inference.

By offering models across various quantization levels, Red Hat AI empowers users to select the precision that best balances their specific performance, accuracy, and cost requirements for their target deployment environment.

=== 3.2. Common Quantization Types and Their Implications

As discussed in previous sections, the primary types of quantization leveraged for LLM optimization include:

* **FP8 (8-bit Floating Point):**
    * *Benefits:* Offers an excellent balance of accuracy and performance on modern NVIDIA GPUs with dedicated FP8 support. Its floating-point nature generally provides a higher dynamic range than INT8 for the same bit width, making it robust for quantizing both weights and activations without significant accuracy degradation, especially for certain model types.
    * *Considerations:* Requires specific hardware acceleration to fully realize its benefits.
* **INT8 (8-bit Integer):**
    * *Benefits:* Widely supported across various hardware. Provides substantial memory and speed benefits over FP16/FP32. Often a good "default" choice when FP8 hardware isn't available or for models less sensitive to integer precision.
    * *Considerations:* Can be more prone to accuracy loss for certain models or activation distributions compared to FP8.
* **INT4 (4-bit Integer):**
    * *Benefits:* Maximum memory reduction and potential for highest throughput due to minimal data transfer.
    * *Considerations:* Highest risk of accuracy degradation. Requires careful techniques (e.g., AWQ, GPTQ) to mitigate this loss. Performance benefits are highly dependent on the inference engine and hardware's ability to efficiently process 4-bit data (often involves packing/unpacking to 8-bit bytes).

== 4. Red Hat AI Validated Model Collections

Red Hat AI organizes its validated models into collections, often featuring various quantized versions of popular LLMs.

=== 4.1. v1.0 Collection of Red Hat AI Validated Models

This initial collection features leading third-party generative AI models rigorously validated for efficient use across the Red Hat AI Product Portfolio. Each model listed below has associated configurations for various quantization levels (e.g., `w4a16` for 4-bit weights and 16-bit activations, `w8a8` for 8-bit weights and 8-bit activations, or `FP8-dynamic` for dynamic FP8 quantization).

* Gemma-3 Quantized
* Whisper Quantized (Note: Whisper is typically an ASR model, but its text-generation component is relevant)
* Llama 4 Quantized
* Qwen3 Quantized
* Mistral Small-3.1 Instruct Quantized
* Phi-4 Quantized
* Llama 3.3 70B Instruct Quantized
* Qwen 2.5 Quantized
* Granite Quantized

[NOTE]
The specific quantization scheme (e.g., `w4a16`, `w8a8`, `FP8-dynamic`) for each model is typically indicated in its full name or metadata within the Red Hat AI Hugging Face repository. These notations signify the precision used for weights (w) and activations (a), or the dynamic FP8 method.

=== 4.2. Availability and Selection Criteria

While Red Hat AI focuses on validated models, other models may also be available in the repository but have not undergone the full validation process. For practical deployment in this course, we will focus on specific pre-selected models from the validated collection.

For internal Red Hat validation and scenario planning, a randomized selection process is used for models that meet certain criteria, such as:

* Functionality on NVIDIA L4 GPUs (with 24GB VRAM).
* Demonstration of a total cost of ownership (TCO) within a defined yearly budget (e.g., < $75,000).
* Ability to show tangible business impact (e.g., improving ticket closure speed by engineering team members by 25%).

These criteria reflect real-world enterprise constraints and use cases, ensuring the validated models are relevant for your work with customers.

== 5. Pre-Selected Models for This Course

For the practical exercises and demonstrations in this course, we will utilize the following specific validated and quantized models:

* `Mistral-Small-3.1-24B-Instruct-2503-quantized.w4a16`
* `RedHatAI/Qwen2.5-VL-3B-Instruct-quantized.w8a8`
* `RedHatAI/gemma-3-4b-it-quantized.w4a16`

These models offer a representative sample of different architectures, sizes, and quantization levels, allowing for hands-on experience with diverse deployment scenarios.

== 6. Next Steps: Deep Dive and Deployment

In subsequent sections, we will:

* **Evaluate each pre-selected model in-depth:** Discuss their specific characteristics, performance profiles, and suitable use cases.
* **Select a model to deploy:** Guide you through the process of choosing and deploying one of these models for hands-on lab activities.

