= Course Module: Real-World Use Case - The Support Ticket Triage Assistant

Welcome to the hands-on portion of our course. In this series of modules, we will move from theory to practice by building a solution for a common customer scenario. As a delivery engineer or consultant, you will frequently be asked to demonstrate the value of AI quickly and efficiently. This use case is designed to equip you with the skills to do just that.

== 1. The Scenario: A Customer's Challenge

Our customer, a growing enterprise software company, is facing a challenge in their support department. Their support engineers spend a significant amount of time on initial ticket triage: reading the ticket, categorizing it, searching for relevant knowledge base articles, and writing an initial summary. This manual process slows down response times and is a repetitive, low-value task for skilled engineers.

They believe an AI-powered assistant could accelerate this process but are hesitant to invest in a large-scale project without proof of its value and technical feasibility in their own environment.

== 2. The Objective: A Proof-of-Concept (PoC)

Our goal is to design and deploy a **Support Ticket Triage Assistant** as a Proof-of-Concept. This PoC must satisfy two primary constraints:

* **Prove Technical Feasibility:** Demonstrate that a Large Language Model (LLM) can be effectively deployed and integrated within their existing infrastructure to deliver tangible value.
* **Minimize Initial Investment:** The entire PoC must be completed within a limited budget of **~$10,000**, covering cloud credits and approximately 2-3 weeks of focused engineering time.

== 3. The Solution: A Human-in-the-Loop MVP

To achieve the objective safely and build trust, we will architect a **Human-in-the-Loop** Minimum Viable Product (MVP). This approach ensures that the AI *assists* the support engineer rather than automating them out of the process.

.System Workflow
image::workflow_diagram.png[System Workflow]

Here's how it will function:

.   A customer submits a new support ticket in Jira.
.   An application, triggered by the new ticket, sends the ticket's title and description to our AI service.
.   Our vLLM-powered model processes the text and generates a structured JSON output containing:
    ** A predicted `category` (e.g., "Billing," "Bug Report," "Feature Request").
    ** A concise `summary` of the issue.
    ** A list of suggested `knowledge_base_urls` for troubleshooting.
.   The application posts this JSON payload as a *private comment* within the Jira ticket.
.   The support engineer opens the ticket and immediately sees the AI-generated triage information, ready for validation and use.

This design provides immediate value by reducing manual work while keeping the engineer in full control of the final response to the customer.

== 4. Technical Environment & Constraints

A critical part of any engagement is working within the customer's existing technical landscape.

* **Platform:** The customer uses **Red Hat OpenShift AI**. Our solution must be deployable and manageable within this environment.
* **Hardware:** They have allocated two compute nodes for this PoC, each equipped with an **NVIDIA A10G GPU** containing 24GB of VRAM.
* **Model Sizing:** This hardware configuration imposes a constraint on model size. A common rule of thumb is that a model requires at least twice its parameter count in GPU memory for weights and KV cache.
    * *Calculation:* `24GB VRAM per GPU`
    * *Constraint:* To run efficiently on a single A10G, we should target models significantly smaller than 12B parameters.
    * *Decision:* For this initial PoC, a smaller, fine-tuned model (e.g., in the 1B to 7B parameter range) is ideal. It will provide fast inference and can be highly effective when provided with good context from past ticket data.

== 5. Division of Responsibilities

Clear boundaries are key to a successful PoC.

* **Our Team (The vLLM Experts):**
    * Select an appropriate open-source text-generation model.
    * Optimize and deploy the model using **vLLM** on OpenShift AI to ensure high-throughput and low-latency inference.
    * Provide a stable, documented API endpoint for the AI service.
    * Focus on model performance, scalability, and deployment best practices.

* **The Customer's Team:**
    * Develop the application logic that acts as the "glue."
    * Handle the Jira integration: using the Jira API to monitor for new tickets (e.g., via webhooks) and to post the private comments.

== 6. Next Steps: Model Selection

With the requirements and architecture defined, our immediate next step is to identify the best candidate model for the job. We will begin by exploring the curated models available in the **Red Hat AI Model Library on Hugging Face** to find a suitable starting point for our Support Ticket Triage Assistant.

'''
```

== Phase 2: Rapid Viability Assessment & PoC (The "How" and "If")

Objective: Prove the "Support Ticket Triage Assistant" is technically feasible and will work in their environment, without over-investing.
Budget Allocation: ~$10,000 (for cloud credits and 2-3 weeks of engineering time).

Design the MVP (Human-in-the-Loop): Instead of full automation, you design a safer, more helpful MVP. The AI will not respond to customers directly. Instead, when a support engineer opens a ticket in Jira, the AI's analysis (category, suggested URL, and a draft summary) will automatically appear in a private comment field.


Need a summarization model or text to text based model for text generation. 
The actual model that could meet these requirements is pretty broad. 
With our knowledge of the their existing hardware of two nodes with GPUs both A10Gs 24GB video memory means we could possible host a model up to ~12B parameters, working with calculation that a model requires 2x its parameters in GPU memory to operator and more for KV caching or other features. 
With this assumption, we can expect to select a model with less than 12B parameters and for this project we could probably go with a really small model since we can use data from previous ticket help align the model or at least provide additional context for model queries.  

Design would be something like a new ticket created in Jira for support,  an API that collects the data in json format and submits it to our AI model for summarization, categorization, and to determine the best links for additional information on the topic. 

Model format the output as json and the application returns this to the jira application which post the summarization and links to urls within the comment field.

What this group will deliver is an deploy AI model in OpenShift AI that provides and endpoint for the customer application that will query Jira, construct the prompt, query the AI model, collect the response, return the response to the application which will in turn post the response to Jira.

The customer will be responsible for the application and Jira integration and we will focus on AI model selection, deployment, performance, along with safety and security coming in later course modules.

We understand the requirements, the next step is to find and select a model as our first test candidate.

To narrow down the model selection process, we are going to review a few of the models in the red hat ai model repository on huggingface. 


