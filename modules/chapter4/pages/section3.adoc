= vLLM Serving Runtime on Red Hat OpenShift AI

This section details how to integrate and utilize the vLLM inference engine as a serving runtime within the Red Hat OpenShift AI platform's Single-Model Serving stack. This allows you to deploy Large Language Models (LLMs) with vLLM's high-performance capabilities, offering an alternative to standard runtimes like Caikit+TGIS or standalone TGIS.

For a comprehensive list of models currently supported by vLLM, refer to the official documentation: https://docs.vllm.ai/en/latest/models/supported_models.html[vLLM Supported Models, window=_blank].

[[gpu-cpu-considerations]]
== 1. GPU vs. CPU Considerations

While vLLM is primarily optimized for GPU acceleration, offering custom CUDA kernels for maximum throughput, it also provides a CPU-only version for scenarios where GPUs are unavailable or for specific development and testing needs.

* **GPU-Accelerated vLLM (Recommended for Production):**
    * **Requirement:** A **GPU is mandatory** to load and run models efficiently due to its reliance on custom CUDA kernels.
    * **Performance:** Designed for high-throughput and low-latency inference, leveraging the parallel processing power of GPUs.
    * **Memory:** Requires sufficient GPU VRAM (Video RAM) to load the model weights. Note that standard RAM is still utilized for pre-loading operations before the model resides fully on the GPU.

* **CPU-Only vLLM (For Specific Use Cases):**
    * **Requirement:** No GPU needed; models run entirely on the CPU.
    * **Limitations:**
        * **Performance:** Generally slower than GPU-accelerated vLLM, suitable for smaller models, development, or specific non-performance-critical scenarios.
        * **Data Type Compatibility:** The CPU version currently has limitations regarding supported data types. It is primarily compatible with FP32 and BF16. Any FP16 model will be automatically converted to BF16 upon loading. For detailed information on CPU limitations and supported features, consult the official vLLM CPU installation documentation: https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html[vLLM CPU Installation, window=_blank].

== 2. Installation and Configuration

Before adding the vLLM runtime, ensure that the necessary components of the Red Hat OpenShift AI Single-Model Serving stack are properly installed and configured. Refer to the official Red Hat documentation for this prerequisite: https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2-latest/html/serving_models/serving-large-models_serving-large-models[Serving Large Models on OpenShift AI, window=_blank].

Once the Single-Model Serving stack is ready, adding the vLLM serving runtime is a straightforward process for an administrator:

.  As an administrator, log in to the OpenShift AI Dashboard.
.  Navigate to `Settings` -> `Serving runtimes` in the left-hand menu.
.  Click on the `Add serving runtime` button.
.  For the "Type of model serving platforms this runtime supports," select `Single model serving platform`.
.  You have two options to define the runtime configuration:
    * **Upload File:** Upload the provided `vllm-runtime.yaml` file from your current working directory.
    * **Start from scratch:** Click `Start from scratch` and copy/paste the content of the `vllm-runtime.yaml` file into the editor.
    * [NOTE]
        A dedicated CPU-only version of the runtime is typically available in a separate YAML file (e.g., `vllm-runtime-cpu.yaml`), which should be used for CPU deployments.

The vLLM serving runtime will now be available for selection when deploying a new model.

== 3. Model Deployment with vLLM

Deploying a model using the vLLM runtime follows the standard procedures within the OpenShift AI Dashboard:

.  **Prepare Model Files:** Ensure your trained LLM files are copied to an accessible S3-compatible object storage bucket.
.  **Initiate Deployment:** From the OpenShift AI Dashboard, navigate to your project and initiate a new model deployment.
.  **Select Runtime:** During the deployment process, select the `vLLM` serving runtime that you added in the previous step.
.  **Resource Allocation:**
    * **GPU Deployments:** Verify that you have configured a GPU for your deployment and allocated sufficient *GPU VRAM* to accommodate the model. Also, ensure adequate *standard memory (RAM)* is provisioned, as it's used for initial model pre-loading operations before the model is fully resident on the GPU.
    * **CPU Deployments:** For CPU-only deployments, ensure sufficient standard RAM and CPU resources are allocated. Be mindful of the CPU-specific data type limitations mentioned in Section 1.

.  **Monitor Deployment:** Monitor the deployment status from the dashboard. Once the model is successfully loaded and the serving instance is ready, the inference endpoint will become accessible.

== 4. Consuming the vLLM Inference Endpoint

One of vLLM's significant advantages for application integration is its **OpenAI-compatible API**. This means any tool or library capable of connecting to OpenAI services can seamlessly consume the vLLM inference endpoint.

* **API Endpoints:**
    vLLM's OpenAI-compatible server implements key OpenAI API endpoints, including:
    * `/v1/completions` (for text generation)
    * `/v1/chat/completions` (for conversational models)
    * `/v1/embeddings` (for embedding models)

* **Development Examples:**
    * **Official Quickstart:** Refer to the vLLM documentation for Python and `curl` examples demonstrating how to interact with the OpenAI-compatible API: https://docs.vllm.ai/en/latest/getting_started/quickstart.html#using-openai-completions-api-with-vllm[vLLM OpenAI Quickstart, window=_blank].
    * **LangChain Integration:** A notebook example demonstrating how to query a vLLM endpoint using the LangChain framework is available in this repository: `../../examples/notebooks/langchain/Langchain-vLLM-Prompt-memory.ipynb`.

* **Interactive API Documentation (Swagger UI):**
    For direct API exploration and testing without writing code, vLLM provides a built-in Swagger UI. It offers comprehensive documentation of all API methods, parameters, and allows you to test endpoints directly from your browser.
    * **Access:** This UI is typically accessible at the address: `https://your-model-inference-endpoint-address/docs`. (Replace `your-model-inference-endpoint-address` with the actual endpoint URL provided by the OpenShift AI Dashboard for your deployed model.)