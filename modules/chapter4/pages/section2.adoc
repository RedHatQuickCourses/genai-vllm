= Individual Model Analysis: Pre-Selected for This Course

In this section, we will conduct a detailed analysis of the three generative AI models pre-selected for this course. These models are sourced from the Red Hat AI repository on Hugging Face, chosen for their comprehensive "Model Card" or "Model Overview" information, and their distinct characteristics relevant to various enterprise deployment scenarios with vLLM.

Each analysis will cover the model's architecture, scale, specific quantization strategy, capabilities, and key performance indicators, providing the necessary context for informed deployment decisions.

== 1. RedHatAI/Mistral-Small-24B-Instruct-2501-FP8-dynamic

* **Hugging Face Link:** https://huggingface.co/RedHatAI/Mistral-Small-24B-Instruct-2501-FP8-dynamic[RedHatAI/Mistral-Small-24B-Instruct-2501-FP8-dynamic, window=_blank]

* **Model Architecture:** This model is based on the highly regarded `Mistral-Small-24B-Instruct-2501` architecture from Mistral AI. It is an instruction-tuned, text-to-text transformer model known for its strong performance in instruction-following and general text generation tasks.

* **Scale:** It is a substantial model featuring approximately *23.6 billion parameters*. This places it in the "small" (for LLMs) to "medium" category, offering a balance between performance and resource consumption.

* **Quantization Strategy (Efficiency):**
    * **Type:** FP8 (8-bit Floating Point) for both weights and activations (`FP8-dynamic`).
    * **Implication:** This optimization significantly reduces the model's memory footprint and disk size by approximately 50% compared to its FP16 counterpart (reducing each parameter from 16 bits to 8 bits). FP8 quantization is particularly effective on modern NVIDIA GPU architectures (e.g., Ada Lovelace, Hopper, L4, H100) that feature dedicated Tensor Cores for accelerated FP8 computations, leading to substantial inference speedups while striving to maintain accuracy.

* **Capabilities:**
    * **Modality:** Text-to-Text. It processes text inputs and generates text outputs.
    * **Use Cases:** Primarily designed for general text generation, complex instruction following, and conversational AI scenarios. It excels in tasks requiring robust language understanding and generation, such as chatbots, content creation, and summarization.
    * **Deployment Readiness:** Explicitly optimized and ready for efficient inference using the vLLM backend. It supports deployment across the Red Hat AI product portfolio, including the Red Hat AI Inference Server, Red Hat Enterprise Linux AI (via `ilab`), and Red Hat OpenShift AI (via KServe).

* **Performance (Accuracy Recovery):**
    * **OpenLLM Benchmark (v1):** Achieved an impressive average score of 78.88, demonstrating a *99.28% accuracy recovery* compared to the unquantized model's 79.45. Individual metrics (ARC-Challenge, GSM8K, HellaSwag, MMLU, TruthfulQA, Winogrande) showed minimal degradation.
    * **OpenLLM Leaderboard (v2):** Scored an average of 51.73, representing a *98.68% recovery* from the unquantized model's 52.42. Note that results for Math-Hard, GPQA, and MUSR were excluded from recovery calculations due to near-random prediction accuracy in the unquantized baseline.
    * **Overall:** The high accuracy recovery rates across multiple benchmarks underscore the effectiveness of the FP8 quantization for this model, making it a strong candidate for production use where both performance and minimal accuracy impact are critical.

== 2. RedHatAI/Qwen2.5-VL-3B-Instruct-quantized.w8a8

* **Hugging Face Link:** https://huggingface.co/RedHatAI/Qwen2.5-VL-3B-Instruct-quantized.w8a8[RedHatAI/Qwen2.5-VL-3B-Instruct-quantized.w8a8, window=_blank]

* **Model Architecture:** This model is a quantized version of the `Qwen/Qwen2.5-VL-3B-Instruct` architecture. The Qwen2.5-VL series are multimodal Large Vision-Language Models (LVLMs) developed by Alibaba Cloud, combining a Vision Transformer (ViT) encoder with a Qwen2.5-series LLM decoder to process both visual and textual information.

* **Scale:** It is a relatively *compact model* with approximately *4.07 billion parameters*. Its smaller size makes it suitable for deployments with tighter memory constraints or when a higher number of concurrent models is required.

* **Quantization Strategy (Efficiency):**
    * **Type:** INT8 (8-bit Integer) for both weights and activations (`w8a8`).
    * **Implication:** This `w8a8` scheme, obtained using `llm-compressor` with a GPTQModifier, provides significant memory savings and computational speedups. Benchmarks indicate speedups of *up to 1.33x in single-stream deployment* and *up to 1.37x in multi-stream asynchronous deployment*, depending on hardware and specific use-case scenarios. It also demonstrates improved "Queries Per Dollar" (QPD) efficiency across various NVIDIA GPUs (e.g., A6000, A100, H100) for multimodal tasks. This makes it highly cost-effective for relevant workloads.

* **Capabilities:**
    * **Modality:** Image-Text-to-Text. This model is truly multimodal, capable of taking both vision (image) and text as input and generating text as output.
    * **Use Cases:** Excellently suited for a wide range of multimodal tasks including:
        * Document Visual Question Answering (DocVQA)
        * Visual Reasoning
        * Image Captioning
        * Chart Analysis
        * General visual understanding and question answering.
    * **Deployment Readiness:** Optimized for efficient inference with vLLM version 0.5.2 or later, making it a strong choice for multimodal applications on Red Hat platforms.

* **Performance (Accuracy Recovery):**
    * **Vision Tasks (MMMU, VQAv2, DocVQA, ChartQA, Mathvista):** Achieved an impressive average score of 69.24, demonstrating a remarkable *99.94% accuracy recovery* compared to the original model's 69.28. This indicates near-lossless quantization for visual understanding.
    * **Text-based Tasks (MGSM, MMLU):** Showed strong performance with MGSM scoring 41.98 (96.09% recovery) and MMLU scoring 64.83 (99.25% recovery).
    * **Overall:** The Qwen2.5-VL-3B-Instruct-quantized.w8a8 model stands out for its high accuracy retention across challenging multimodal benchmarks while offering substantial efficiency gains through INT8 quantization.

== 3. RedHatAI/gemma-3-4b-it-quantized.w4a16

* **Hugging Face Link:** https://huggingface.co/RedHatAI/gemma-3-4b-it-quantized.w4a16[RedHatAI/gemma-3-4b-it-quantized.w4a16, window=_blank]

* **Model Architecture:** This model is a quantized variant of `google/gemma-3-4b-it`. Gemma models are a family of lightweight, state-of-the-art open models built by Google DeepMind. The `-it` suffix indicates it's an instruction-tuned variant, optimized for dialogue and instruction following. It is also a multimodal model, capable of handling image and text inputs.

* **Scale:** This is the *smallest of the reviewed models*, with approximately *1.61 billion parameters*. Its highly compact size makes it exceptionally suitable for resource-constrained environments, edge deployments, or scenarios where minimizing GPU memory usage is paramount.

* **Quantization Strategy (Efficiency):**
    * **Type:** INT4 (4-bit Integer) for weights (`w4`) and FP16 (16-bit Floating Point) for activations (`a16`).
    * **Implication:** This `w4a16` scheme, prepared using `llm-compressor` and GPTQModifier, provides the highest level of weight compression among the selected models, drastically reducing the model's disk footprint and loading size. While activations remain at FP16 for better accuracy, the 4-bit weights contribute significantly to memory savings and potential throughput improvements, especially for workloads with low queries per second (QPS) and latency sensitivity. It is optimized for inference with vLLM version 0.8.0 or later.

* **Capabilities:**
    * **Modality:** Image-Text-to-Text. Similar to the Qwen model, it processes both image and text inputs to generate textual outputs.
    * **Use Cases:** Well-suited for multimodal applications where model size and efficiency are critical, such as visual question answering on devices with limited memory, or for batch processing where maximizing the number of concurrent models is desired. Its instruction-tuned nature also makes it capable of conversational tasks.

* **Performance (Accuracy Recovery):**
    * **OpenLLM v1 Text Benchmark:** Achieved an average score of 63.04%, showing a *97.42% accuracy recovery* from the original model's 64.70%. This includes individual scores for ARC Challenge, GSM8K, Hellaswag, MMLU, TruthfulQA, and Winogrande.
    * **Vision Evals (MMMU and ChartQA):** Recorded an average score of 44.72%, demonstrating a *98.86% accuracy recovery*.
    * **Overall:** Despite its aggressive 4-bit weight quantization, the Gemma-3-4b-it-quantized.w4a16 model maintains a remarkably high accuracy recovery across both text and vision tasks, making it an excellent choice for highly efficient multimodal deployments.

== 4. Rationale for Model Selection in This Course

The selection of these three specific models for hands-on activities in this course is based on a strategic rationale to provide a comprehensive and practical learning experience for delivery engineers and consultants:

* **Direct Alignment with vLLM Deployment Objectives:** All three highlighted Red Hat AI models are explicitly designed, optimized, and validated for efficient inference using the vLLM backend. Red Hat AI even utilizes vLLM for inference serving as a core part of its model validation process, providing an extra layer of confidence in their compatibility and performance within a vLLM environment. This ensures that your lab experiences directly reflect real-world optimal deployment practices.

* **Prioritization of Efficiency via Diverse Quantization:** These models collectively showcase the practical application and benefits of various advanced quantization techniques: FP8, INT8 (`w8a8`), and INT4 (`w4a16`). This diversity allows you to:
    * Understand the trade-offs between different precision levels.
    * Observe how these optimizations (reducing model size, disk footprint, and GPU memory requirements) directly contribute to cost-efficient deployments and improved "Queries Per Dollar" (QPD).
    * Gain hands-on experience with models tailored for different hardware capabilities and performance requirements.

* **Demonstration of Diverse Capabilities and Scale:**
    * The `Mistral-Small-24B-Instruct-2501-FP8-dynamic` represents a larger, text-only instruction-tuned model, ideal for robust general-purpose LLM applications.
    * The `Qwen2.5-VL-3B-Instruct-quantized.w8a8` and `gemma-3-4b-it-quantized.