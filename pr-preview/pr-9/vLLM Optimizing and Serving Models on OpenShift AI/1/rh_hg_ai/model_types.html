<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Generative AI Model Variations in Model Naming :: vLLM Optimizing and Serving Models on OpenShift AI</title>
    <link rel="prev" href="index.html">
    <link rel="next" href="val_models.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">vLLM Optimizing and Serving Models on OpenShift AI</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="vLLM Optimizing and Serving Models on OpenShift AI" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../vllm/index.html">vLLM, What is it?</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_intro.html">vLLM</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/rh_ai.html">Red Hat AI Platforms</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_rhoai.html">Integrating vLLM with OpenShift AI: The Serving Runtime</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_deploy.html">vLLM on Red Hat OpenShift AI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_concl.html">Summary: Key Takeaways and Next Steps</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../model_sizing/index.html">GPU Architecture and Model Sizing</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/gpu_arch.html">NVIDIA GPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/gpu_cost.html">Estimating GPU VRAM</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/vram_calc.html">Cost-Effective Model Selection</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Red Hat AI Model Repository</a>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="model_types.html">Generative AI Model Variations in Model Naming</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="val_models.html">Validated Models and Quantization Strategies</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="lab_models.html">Pre-Selected Course Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="summary.html">From Curation to Confident Deployment</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../rhoai_deploy/index.html">OpenShift AI Configuration</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_self.html">Optional - RHOAI Self-Managed</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_221.html">Lab Environment Customization</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_config.html">Project and Data Connection Setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_bench.html">Creating the AI Workbench and Preparing Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/add_runtime.html">vLLM Serving Runtime</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_model.html">Deploy Granite LLM on RHOAI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_query.html">Querying the Deployed Model in a Jupyter Notebook</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/batch_summ.html">Using the AI Model for Batch Summarization</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../rhoai_query/index.html">Jupyter Notebooks &amp; LLMs</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_query/section1.html">Jupyter Notebooks</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_query/section2.html">Mistral LLM Model Inference</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_query/section3.html">Llama3 LLM Model Inference</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/further_study.html">Hands on Exploration and Further Study</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../LABENV/index.html">Lab Environments</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/index-lab-demo.html">Red Hat Demo Hub: Lab Environment Selection</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/developer_sandbox.html">Red Hat Developer Platform</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">vLLM Optimizing and Serving Models on OpenShift AI</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a></li>
    <li><a href="index.html">Red Hat AI Model Repository</a></li>
    <li><a href="model_types.html">Generative AI Model Variations in Model Naming</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Generative AI Model Variations in Model Naming</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Welcome to the next chapter of our course on model inference with vLLM. Before we dive into the Red Hat AI model repository on Hugging Face, it&#8217;s crucial to understand the common variations you&#8217;ll encounter in model names. These suffixes and descriptors aren&#8217;t arbitrary; they signify the model&#8217;s training, capabilities, and intended use case. This page will demystify these common variations.</p>
</div>
<div class="paragraph">
<p>The generative AI landscape is filled with a diverse set of models, each optimized for specific tasks. Understanding their lineage helps in selecting the right tool for the job.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_1_base_models"><a class="anchor" href="#_1_base_models"></a>1. Base Models</h2>
<div class="sectionbody">
<div class="dlist">
<dl>
<dt class="hdlist1"><strong>What it is</strong></dt>
<dd>
<p>A <strong>base model</strong>, often called a "foundational model," is the result of the initial, resource-intensive pre-training phase. It&#8217;s an LLM trained on a vast and general corpus of text and code, forming the foundation from which more specialized models are derived.</p>
</dd>
<dt class="hdlist1"><strong>Key Characteristics</strong></dt>
<dd>
<div class="ulist">
<ul>
<li>
<p><strong>Broad, Unstructured Knowledge:</strong> Possesses extensive general knowledge from its training data.</p>
</li>
<li>
<p><strong>Next-Token Prediction:</strong> Its fundamental function is to predict the next token in a sequence. It doesn&#8217;t inherently understand "instructions" or "dialogue."</p>
</li>
<li>
<p><strong>Unrefined Output:</strong> Interacting with a raw base model can be unpredictable. Its output is a continuation of the input prompt based on patterns learned during training, not necessarily a direct or helpful answer.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1"><strong>Analogy</strong></dt>
<dd>
<p>Think of a base model as a massive, raw database of human language. It contains the information, but there&#8217;s no query engine or user-friendly interface to retrieve and structure that information in a helpful way.</p>
</dd>
</dl>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_2_instruct_instruction_tuned_models"><a class="anchor" href="#_2_instruct_instruction_tuned_models"></a>2. Instruct (Instruction-Tuned) Models</h2>
<div class="sectionbody">
<div class="dlist">
<dl>
<dt class="hdlist1"><strong>What it is</strong></dt>
<dd>
<p>An <strong>instruct model</strong> is a base model that has undergone a second training phase known as instruction fine-tuning. This process, often called "alignment," trains the model on a curated dataset of <code>(instruction, response)</code> pairs. This phase frequently uses techniques like Reinforcement Learning with Human Feedback (RLHF) to align the model&#8217;s outputs with human preferences for helpfulness and safety.</p>
</dd>
<dt class="hdlist1"><strong>Key Characteristics</strong></dt>
<dd>
<div class="ulist">
<ul>
<li>
<p><strong>Instruction Following:</strong> Its primary strength is understanding and executing specific user commands.</p>
</li>
<li>
<p><strong>Helpful Assistant Persona:</strong> It&#8217;s trained to behave like a cooperative assistant, providing direct answers to questions.</p>
</li>
<li>
<p><strong>Improved Safety and Predictability:</strong> The alignment process makes the model more reliable and less likely to generate undesirable content compared to its base version. Most public-facing chatbots (like ChatGPT, Gemini, and Claude) are instruction-tuned models.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1"><strong>Analogy</strong></dt>
<dd>
<p>The raw database now has a powerful query engine and an API. You can submit a specific query (an instruction), and it returns a structured, relevant, and helpful response.</p>
</dd>
</dl>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_3_chat_models"><a class="anchor" href="#_3_chat_models"></a>3. Chat Models</h2>
<div class="sectionbody">
<div class="dlist">
<dl>
<dt class="hdlist1"><strong>What it is</strong></dt>
<dd>
<p>A <strong>chat model</strong> is a specialized type of instruct model that is further optimized for multi-turn dialogue. While most instruct models can handle a conversational back-and-forth, chat models are explicitly fine-tuned to maintain context over longer interactions, leading to more coherent and natural-feeling conversations.</p>
</dd>
<dt class="hdlist1"><strong>Key Characteristics</strong></dt>
<dd>
<div class="ulist">
<ul>
<li>
<p><strong>Dialogue Coherence:</strong> Excels at tracking the conversational history to inform its next response.</p>
</li>
<li>
<p><strong>Contextual Awareness:</strong> Superior ability to handle follow-up questions, clarifications, and references to earlier points in the conversation.</p>
</li>
<li>
<p><strong>Enhanced User Experience:</strong> Provides a more seamless and intuitive conversational flow.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1"><strong>Analogy</strong></dt>
<dd>
<p>This is a stateful API. It not only responds to your current query but also remembers all your previous queries within the same session, allowing for a continuous and context-aware interaction.</p>
</dd>
</dl>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_4_code_models"><a class="anchor" href="#_4_code_models"></a>4. Code Models</h2>
<div class="sectionbody">
<div class="dlist">
<dl>
<dt class="hdlist1"><strong>What it is</strong></dt>
<dd>
<p>A <strong>code model</strong> is a specialized LLM focused on software development tasks. It is either pre-trained from the ground up on a massive corpus of public source code or is a base model that has been extensively fine-tuned on code-specific datasets.</p>
</dd>
<dt class="hdlist1"><strong>Key Characteristics</strong></dt>
<dd>
<div class="ulist">
<ul>
<li>
<p><strong>Programming Expertise:</strong> Designed for code generation, completion, debugging, and explanation.</p>
</li>
<li>
<p><strong>Syntactic and Semantic Understanding:</strong> Possesses a deep grasp of programming language syntax, libraries, APIs, and common software architecture patterns. While a general instruct model can generate simple code, a dedicated code model produces more accurate, efficient, and idiomatic results.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1"><strong>Analogy</strong></dt>
<dd>
<p>This is a highly specialized technical database and query engine built exclusively for developers. It&#8217;s staffed by an expert programmer who can not only find information but also write and debug complex code.</p>
</dd>
</dl>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_5_task_specific_fine_tuned_models_e_g_summarization"><a class="anchor" href="#_5_task_specific_fine_tuned_models_e_g_summarization"></a>5. Task-Specific Fine-Tuned Models (e.g., Summarization)</h2>
<div class="sectionbody">
<div class="dlist">
<dl>
<dt class="hdlist1"><strong>What it is</strong></dt>
<dd>
<p>Beyond general-purpose tuning, models can be fine-tuned for highly specific tasks. A <strong>summarization model</strong> is a prime example, trained on a dataset of long-form documents paired with their high-quality, concise summaries. This same principle applies to other tasks like translation, sentiment analysis, or classification.</p>
</dd>
<dt class="hdlist1"><strong>Key Characteristics</strong></dt>
<dd>
<div class="ulist">
<ul>
<li>
<p><strong>Domain Expertise:</strong> Highly proficient at its one specific function (e.g., condensing large texts into short, accurate summaries).</p>
</li>
<li>
<p><strong>Superior Performance:</strong> For its designated task, it will almost always outperform a general-purpose instruct model.</p>
</li>
<li>
<p><strong>Abstractive Power:</strong> A specialized summarization model is often better at <strong>abstractive</strong> summarization (generating new sentences to convey the core meaning) rather than purely <strong>extractive</strong> summarization (copying key sentences from the source).</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1"><strong>Analogy</strong></dt>
<dd>
<p>This is a dedicated microservice. Instead of a general-purpose API that can do many things, this is a highly optimized endpoint designed for a single, specific function, like generating a "TL;DR" for a long article. It does one thing, and it does it exceptionally well.</p>
</dd>
</dl>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="index.html">Red Hat AI Model Repository</a></span>
  <span class="next"><a href="val_models.html">Validated Models and Quantization Strategies</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
