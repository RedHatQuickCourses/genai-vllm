<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Deploy Granite LLM on RHOAI :: vLLM Optimizing and Serving Models on OpenShift AI</title>
    <link rel="prev" href="add_runtime.html">
    <link rel="next" href="rhoai_query.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">vLLM Optimizing and Serving Models on OpenShift AI</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="vLLM Optimizing and Serving Models on OpenShift AI" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../vllm/index.html">vLLM, What is it?</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_intro.html">vLLM</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/rh_ai.html">Red Hat AI Platforms</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_rhoai.html">Integrating vLLM with OpenShift AI: The Serving Runtime</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_deploy.html">vLLM on Red Hat OpenShift AI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_concl.html">Summary: Key Takeaways and Next Steps</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../model_sizing/index.html">GPU Architecture and Model Sizing</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/gpu_arch.html">NVIDIA GPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/gpu_cost.html">Estimating GPU VRAM</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/vram_calc.html">Cost-Effective Model Selection</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../rh_hg_ai/index.html">Red Hat AI Model Repository</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/model_types.html">Generative AI Model Variations in Model Naming</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/val_models.html">Validated Models and Quantization Strategies</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/lab_models.html">Pre-Selected Course Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/summary.html">From Curation to Confident Deployment</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">OpenShift AI Configuration</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rhoai_self.html">Optional - RHOAI Self-Managed</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rhoai_221.html">Lab Environment Customization</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rhoai_config.html">Project and Data Connection Setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rhoai_bench.html">Creating the AI Workbench and Preparing Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="add_runtime.html">vLLM Serving Runtime</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="rhoai_model.html">Deploy Granite LLM on RHOAI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rhoai_query.html">Querying the Deployed Model in a Jupyter Notebook</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="batch_summ.html">Using the AI Model for Batch Summarization</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../rhoai_query/index.html">Jupyter Notebooks &amp; LLMs</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_query/section1.html">Jupyter Notebooks</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_query/section2.html">Mistral LLM Model Inference</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_query/section3.html">Llama3 LLM Model Inference</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/further_study.html">Hands on Exploration and Further Study</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../LABENV/index.html">Lab Environments</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/index-lab-demo.html">Red Hat Demo Hub: Lab Environment Selection</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/developer_sandbox.html">Red Hat Developer Platform</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">vLLM Optimizing and Serving Models on OpenShift AI</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a></li>
    <li><a href="index.html">OpenShift AI Configuration</a></li>
    <li><a href="rhoai_model.html">Deploy Granite LLM on RHOAI</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Deploy Granite LLM on RHOAI</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>In the previous segment we uploaded the granite model to our S3 Storage.</p>
</div>
<div class="paragraph">
<p>Now it&#8217;s time to deploy that model in our workbench</p>
</div>
<div class="paragraph">
<p>Let&#8217;s head to the OpenShift AI Dashboard, in our Data Science Project, select the Models tab of our bench.  There are two options Single Model Serving and Multi-Model Serving.</p>
</div>
<div class="paragraph">
<p>We are going use the Single Model Serving Option.</p>
</div>
<div class="paragraph">
<p>In order to deploy our model we&#8217;ll need to specific a details</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>first is " model deployment name" :  This example uses granite for the deployment name.</p>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
It&#8217;s important to note that this name is the name of the model service that will be exposed.
if you connect to the model, it will be known as granite only, not the full model name.
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>the second dropdown option is for serving runtime available.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>select the vLLM NVIDIA GPU servingRuntime Kserve -</p>
</div>
<div class="paragraph">
<p>The framework should be auto populated with vLLM for this runtime, other runtimes have a few options available.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>third option is to set the the number of model replcias for this deployment.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Due the video card requirment, keep this set to one.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>next selection is for the model size</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>These are T-shift size, in Small, Medium, Large, or custom</p>
</div>
<div class="paragraph">
<p>For our lab choose the the large option, even though the walkthrough below will show medium.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Is a checkbox, that say&#8217;s "make deployed model available through an external route".</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>check this box, which is automicmailly select the box to require token authenication.</p>
</div>
<div class="paragraph">
<p>it will also show a new option for a service account, set to the default-name.  Optionally change this to be the model name, just to keep logs and metrics easier to track.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>The next step is to select the source model location.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>first select from existing data connection, or add a new connection.   once the connection has been added, then we can specific the subdirectory where model files reside.</p>
</div>
<div class="paragraph">
<p>7 finally we can add additional serving runtime arguments to execute when loading the model for inference.</p>
</div>
<div class="paragraph">
<p>click deploy</p>
</div>
<div class="paragraph">
<p>Model deploy can take some time as large model files are copied from s3 into the container before they can even be started.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_interactive_demo_vllm_openshift_ai_granite_llm_inference"><a class="anchor" href="#_interactive_demo_vllm_openshift_ai_granite_llm_inference"></a>Interactive Demo: vLLM, OpenShift AI, Granite LLM Inference</h3>
<div class="paragraph">
<p>The following interactive demonstration will walk you through the process of:
1.  Deploy an LLM using the Single Model Serving option in OpenShift AI Workbench
2.  Configure properites needed to deploy a model.
3.  Optionally create a token service account for external model access.</p>
</div>
<div class="paragraph">
<p>Follow the on-screen prompts in the demo to complete the model deployment.</p>
</div>
<iframe
  src="https://demo.arcade.software/xf4enBigdszSusJapXdE?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true"
  width="100%"
  height="600px"
  frameborder="0"
  allowfullscreen
  webkitallowfullscreen
  mozallowfullscreen
  allow="clipboard-write"
  muted>
</iframe>
</div>
<nav class="pagination">
  <span class="prev"><a href="add_runtime.html">vLLM Serving Runtime</a></span>
  <span class="next"><a href="rhoai_query.html">Querying the Deployed Model in a Jupyter Notebook</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
