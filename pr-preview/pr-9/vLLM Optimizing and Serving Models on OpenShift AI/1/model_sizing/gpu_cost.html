<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Estimating GPU VRAM :: vLLM Optimizing and Serving Models on OpenShift AI</title>
    <link rel="prev" href="gpu_arch.html">
    <link rel="next" href="vram_calc.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">vLLM Optimizing and Serving Models on OpenShift AI</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="vLLM Optimizing and Serving Models on OpenShift AI" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../vllm/index.html">vLLM, What is it?</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_intro.html">vLLM</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/rh_ai.html">Red Hat AI Platforms</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_rhoai.html">Integrating vLLM with OpenShift AI: The Serving Runtime</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_deploy.html">vLLM on Red Hat OpenShift AI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_concl.html">Summary: Key Takeaways and Next Steps</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">GPU Architecture and Model Sizing</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="gpu_arch.html">NVIDIA GPU Architecture</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="gpu_cost.html">Estimating GPU VRAM</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="vram_calc.html">Cost-Effective Model Selection</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../rh_hg_ai/index.html">Red Hat AI Model Repository</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/model_types.html">Generative AI Model Variations in Model Naming</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/val_models.html">Validated Models and Quantization Strategies</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/lab_models.html">Pre-Selected Course Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/summary.html">From Curation to Confident Deployment</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../rhoai_deploy/index.html">OpenShift AI Configuration</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_self.html">Optional - RHOAI Self-Managed</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_221.html">Lab Environment Customization</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_config.html">Project and Data Connection Setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_bench.html">Creating the AI Workbench and Preparing Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/add_runtime.html">vLLM Serving Runtime</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_model.html">Deploy Granite LLM on RHOAI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_query.html">Querying the Deployed Model in a Jupyter Notebook</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/batch_summ.html">Using the AI Model for Batch Summarization</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../rhoai_query/index.html">Jupyter Notebooks &amp; LLMs</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_query/section1.html">Jupyter Notebooks</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_query/section2.html">Mistral LLM Model Inference</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_query/section3.html">Llama3 LLM Model Inference</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/further_study.html">Hands on Exploration and Further Study</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../LABENV/index.html">Lab Environments</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/index-lab-demo.html">Red Hat Demo Hub: Lab Environment Selection</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/developer_sandbox.html">Red Hat Developer Platform</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">vLLM Optimizing and Serving Models on OpenShift AI</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a></li>
    <li><a href="index.html">GPU Architecture and Model Sizing</a></li>
    <li><a href="gpu_cost.html">Estimating GPU VRAM</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Estimating GPU VRAM</h1>
<div class="sect1">
<h2 id="_cost_aware_model_and_hardware_selection"><a class="anchor" href="#_cost_aware_model_and_hardware_selection"></a>Cost-Aware Model and Hardware Selection</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In the previous sections, we focused on the vLLM engine and its deployment on OpenShift AI. Now, we will shift our focus to the economics of AI infrastructure. This module will equip you with the knowledge to make cost-effective decisions by aligning LLM selection, compression techniques, and hardware choices.</p>
</div>
<div class="sect2">
<h3 id="_the_vram_blueprint_estimating_true_memory_costs"><a class="anchor" href="#_the_vram_blueprint_estimating_true_memory_costs"></a>The VRAM Blueprint: Estimating True Memory Costs</h3>
<div class="paragraph">
<p>The first step in cost-effective deployment is accurately estimating your GPU memory (VRAM) requirements. Under-provisioning leads to out-of-memory errors and service downtime, while over-provisioning leads to idle, expensive hardware and wasted budget. This section provides a blueprint for calculating the <strong>true</strong> VRAM footprint of a model in production.</p>
</div>
</div>
<div class="sect2">
<h3 id="_the_baseline_cost_model_weights"><a class="anchor" href="#_the_baseline_cost_model_weights"></a>The Baseline Cost: Model Weights</h3>
<div class="paragraph">
<p>The most straightforward VRAM cost is the space needed to load the model&#8217;s parameters (weights). This is a function of two variables: the size of the model (in billions of parameters) and the numerical precision used to store each parameter.</p>
</div>
<div class="paragraph">
<p>Using a lower-precision format via <strong>quantization</strong> is the most direct way to reduce this baseline memory cost.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 1. Estimated VRAM for Model Weights by Precision</caption>
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Model Size (Parameters)</th>
<th class="tableblock halign-left valign-top">FP16 / BF16 (16-bit)</th>
<th class="tableblock halign-left valign-top">INT8 (8-bit)</th>
<th class="tableblock halign-left valign-top">INT4 (4-bit)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>1 Billion</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~2 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~1 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~0.5 GB</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>3 Billion</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~6 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~3 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~1.5 GB</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>7 Billion</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~14 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~7 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~3.5 GB</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>13 Billion</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~26 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~13 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~6.5 GB</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>30 Billion</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~60 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~30 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~15 GB</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>70 Billion</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~140 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~70 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~35 GB</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The calculation is simple: VRAM (GB) ≈ (Number of Parameters in Billions) x (Bits per Parameter / 8). A 7B model at FP16 is <code>7 * (16 / 8) = 14 GB</code>.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_the_hidden_costs_beyond_the_weights"><a class="anchor" href="#_the_hidden_costs_beyond_the_weights"></a>The Hidden Costs: Beyond the Weights</h3>
<div class="paragraph">
<p>A common mistake is to select a GPU based only on the model weight VRAM. In a live production environment, several other components consume significant memory, and they must be factored into your budget.</p>
</div>
<div class="imageblock unresolved">
<div class="content">
<img src="vram_components.png" alt="A conceptual stacked bar chart showing that total VRAM is composed of a large block for Model Weights" width="an equally large or larger block for the KV Cache" height="and a smaller block for System Overhead.">
</div>
<div class="title">Figure 1. VRAM Usage Components</div>
</div>
</div>
<div class="sect2">
<h3 id="_1_the_kv_cache_the_memory_hog"><a class="anchor" href="#_1_the_kv_cache_the_memory_hog"></a>1. The KV Cache (The Memory Hog)</h3>
<div class="paragraph">
<p>The Key-Value (KV) Cache stores attention data for the sequence being processed. For modern LLMs, this is often the <strong>largest and most volatile consumer of VRAM</strong>. Its size is not fixed; it grows dynamically based on your workload.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Key Drivers:</strong> The size of the KV Cache is directly proportional to:</p>
</li>
<li>
<p><strong>Batch Size:</strong> The number of requests you process concurrently.</p>
</li>
<li>
<p><strong>Context Length:</strong> The number of tokens (input + output) in each request.</p>
</li>
<li>
<p><strong>Impact:</strong> For applications with long context windows (e.g., document summarization) or high batch sizes, the KV Cache can easily consume <strong>more VRAM than the model weights themselves</strong>. A 14 GB model might require another 16 GB or more for its KV Cache under heavy load.</p>
</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
This is precisely the problem vLLM was designed to solve. Its core innovation, <strong>PagedAttention</strong>, drastically reduces the memory wasted by the KV Cache, allowing you to handle larger batches and longer contexts on the same GPU, directly improving your cost-performance ratio.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_2_cuda_system_overhead"><a class="anchor" href="#_2_cuda_system_overhead"></a>2. CUDA &amp; System Overhead</h3>
<div class="paragraph">
<p>This is the fixed cost of doing business on a GPU. It includes memory consumed by the NVIDIA CUDA kernels, the core PyTorch and vLLM libraries, and various system buffers required to manage the computation.
* <strong>Estimated Cost:</strong> Budget an additional <strong>10-20%</strong> of the model&#8217;s weight VRAM for this overhead.</p>
</div>
</div>
<div class="sect2">
<h3 id="_3_model_activations"><a class="anchor" href="#_3_model_activations"></a>3. Model Activations</h3>
<div class="paragraph">
<p>These are the intermediate values calculated during the model&#8217;s forward pass. While their memory impact is far smaller than the KV Cache, they are a non-zero factor in the total VRAM load.</p>
</div>
</div>
<div class="sect2">
<h3 id="_the_real_world_vram_equation"><a class="anchor" href="#_the_real_world_vram_equation"></a>The Real-World VRAM Equation</h3>
<div class="paragraph">
<p>Therefore, a practical formula for estimating your total memory requirement looks like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-text hljs" data-lang="text">Total VRAM Needed ≈ (VRAM for Model Weights) + (VRAM for Max KV Cache) + (VRAM for System Overhead)</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="title">Don&#8217;t Be Fooled by the "Sticker Price"</div>
<div class="paragraph">
<p>A model&#8217;s advertised size is not its final cost in production. A 13B parameter model might list a ~26 GB requirement for its FP16 weights, suggesting it could fit on a 32 GB GPU. However, with a large batch size and long context window for the KV Cache, the <strong>actual VRAM requirement can easily exceed 40 GB</strong>.</p>
</div>
<div class="paragraph">
<p><strong>Golden Rule:</strong> Always profile your specific use case with realistic batch sizes and context lengths. Never select hardware based solely on the VRAM needed for model weights.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_connecting_vram_to_infrastructure_cost"><a class="anchor" href="#_connecting_vram_to_infrastructure_cost"></a>Connecting VRAM to Infrastructure Cost</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Understanding the VRAM requirements allows you to estimate the annual infrastructure cost. Below are estimates for common GPU instances based on a 1-year commitment.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 2. Annual Cloud Cost Estimates</caption>
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">VRAM per GPUs</th>
<th class="tableblock halign-left valign-top">Example AWS Instance</th>
<th class="tableblock halign-left valign-top">Estimated Annual Cost</th>
<th class="tableblock halign-left valign-top">Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">24 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>g6.4xlarge</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>$7,000 - $8,000</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Recommended starting point for PoCs</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">48 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>g6e.2xlarge</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">$12,000 - $14,000</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Cost doubles from the 24GB tier</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">192 GB (4x48)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>g6e.12xlarge</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">$55,000 - $65,000</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">For multi-model serving or very large models</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">640 GB (8x80)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>p5.48xlarge</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">$240,000+</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Enterprise scale (based on monthly cost)</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect1">
<h2 id="_our_recommended_project_strategy"><a class="anchor" href="#_our_recommended_project_strategy"></a>Our Recommended Project Strategy</h2>
<div class="sectionbody">
<div class="paragraph">
<p>For a typical customer Proof-of-Concept (PoC) with a limited budget, follow this strategic workflow.</p>
</div>
<div class="paragraph">
<div class="title">Target the Sweet Spot</div>
<p>Start by targeting the <strong>24 GB VRAM</strong> infrastructure (<code>g6.4xlarge</code>). This class of GPU offers the best performance-per-dollar and aligns with customer budgets for initial projects.</p>
</div>
<div class="paragraph">
<div class="title">Search with Intent</div>
<p>Filter your model search to those that provide <strong>quantized versions</strong>. A quantized 13B model can often outperform a non-quantized 7B model while fitting in the same 24GB memory budget.</p>
</div>
<div class="paragraph">
<div class="title">Validate and Iterate</div>
<p>Deploy your chosen model and benchmark its performance <strong>and</strong> real-world VRAM consumption. Be prepared to test different models to find the optimal balance of speed, accuracy, and cost for the customer&#8217;s specific use case.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_red_hat_sizing_guide"><a class="anchor" href="#_red_hat_sizing_guide"></a>Red Hat Sizing Guide</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Intended to help provide <strong>a model for estimations for sizing clusters for OpenShift AI</strong> based on a few questions about the customers intended usage.</p>
</div>
<div class="paragraph">
<p>Internal Only - <a href="http://red.ht/rhoai-sizing-guide">OpenShift AI Cluster sizing sheet</a></p>
</div>
<div class="paragraph">
<p>slack channel #help-rhoai-sizing-guide</p>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="gpu_arch.html">NVIDIA GPU Architecture</a></span>
  <span class="next"><a href="vram_calc.html">Cost-Effective Model Selection</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
