<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>vLLM Runtime :: vLLM, From Model to Deployment</title>
    <link rel="prev" href="section2.html">
    <link rel="next" href="section3.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">vLLM, From Model to Deployment</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="vLLM, From Model to Deployment" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">vLLM, From Model to Deployment</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/index.html">Red Hat AI</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/section1.html">The Challenge</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/section2.html">The Scenario</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/openshift_ai_overview.html">Innovatech&#8217;s OpenShift AI Environment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/section3.html">Red Hat AI Platform Solutions</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/section4.html">Model Inference with OpenShift AI</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Chapter 2</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/section1.html">Section 1</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter3/index.html">Red Hat AI Model Repository</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/section1.html">Red Hat AI: Validated Models and Quantization Strategies</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/section2.html">Individual Model Analysis: Pre-Selected for This Course</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/section3.html">Course Module: Real-World Use Case - The Support Ticket Triage Assistant</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">vLLM, What is it ?</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section2.html">Introduction to vLLM</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="section1.html">vLLM Runtime</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section3.html">vLLM Serving Runtime on Red Hat OpenShift AI</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/model_phases.html">Common model variations</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/further_study.html">Hands on Exploration and Further Study</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/further_studyv2.html">Common Generative AI Model Variations and Their Creation</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../LABENV/index.html">Lab Environments</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/index-lab-demo.html">Red Hat Demo Hub: Lab Environment Selection</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/developer_sandbox.html">Red Hat Developer Platform</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">vLLM, From Model to Deployment</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">vLLM, From Model to Deployment</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">vLLM, From Model to Deployment</a></li>
    <li><a href="index.html">vLLM, What is it ?</a></li>
    <li><a href="section1.html">vLLM Runtime</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">vLLM Runtime</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>The <a href="https://docs.vllm.ai/en/latest/index.html">vLLM</a> runtime can be used with Open Data Hub and OpenShift AI Single-Model Serving stack to serve Large Language Models (LLMs) as an alternative to Caikit+TGIS or standalone TGIS. Currently supported models are listed [here](<a href="https://docs.vllm.ai/en/latest/models/supported_models.html" class="bare">https://docs.vllm.ai/en/latest/models/supported_models.html</a>).</p>
</div>
<div class="paragraph">
<p>Note that as the standard runtime is specifically meant to run LLMs and uses custom kernels based on CUDA, a <strong>GPU is required</strong> to load models for this version.
There is, however, also a <strong>CPU version</strong> that can load models on the CPU.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_installation"><a class="anchor" href="#_installation"></a>Installation</h2>
<div class="sectionbody">
<div class="paragraph">
<p>You must first make sure that you have properly installed the necessary components of the Single-Model Serving stack for Openshift AI, as documented [here](<a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2-latest/html/serving_models/serving-large-models_serving-large-models)">here</a>.</p>
</div>
<div class="paragraph">
<p>Once the stack is installed, adding the runtime is pretty straightforward:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>As an admin, in the OpenShift AI Dashboard, open the menu <code>Settings &#8594; Serving runtimes</code>.</p>
</li>
<li>
<p>Click on <code>Add serving runtime</code>.</p>
</li>
<li>
<p>For the type of model serving platforms this runtime supports, select <code>Single model serving platform</code>.</p>
</li>
<li>
<p>Upload the file <code>vllm-runtime.yaml</code> from the current folder, or click <code>Start from scratch</code> and copy/paste its content. A CPU-only version of the runtime is also available in the relevant file.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The runtime is now available when deploying a model.</p>
</div>
<div class="sect2">
<h3 id="_vllm_serving_runtime_for_rhoai"><a class="anchor" href="#_vllm_serving_runtime_for_rhoai"></a>vLLM Serving Runtime for RHOAI</h3>
<details>
<summary class="title">Click to view vLLM Serving Runtime code.</summary>
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># Setting up vllm server with ServingRuntime
# Save as: vllm-servingruntime.yaml
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
 name: vllm-cuda-runtime # OPTIONAL CHANGE: set a unique name
 annotations:
   openshift.io/display-name: vLLM NVIDIA GPU ServingRuntime for KServe
   opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
 labels:
   opendatahub.io/dashboard: 'true'
spec:
 annotations:
   prometheus.io/port: '8080'
   prometheus.io/path: '/metrics'
 multiModel: false
 supportedModelFormats:
   - autoSelect: true
     name: vLLM
 containers:
   - name: kserve-container
     image: quay.io/modh/vllm:rhoai-2.20-cuda # CHANGE if needed. If AMD: quay.io/modh/vllm:rhoai-2.20-rocm
     command:
       - python
       - -m
       - vllm.entrypoints.openai.api_server
     args:
       - "--port=8080"
       - "--model=/mnt/models"
       - "--served-model-name={{.Name}}"
     env:
       - name: HF_HOME
         value: /tmp/hf_home
     ports:
       - containerPort: 8080
         protocol: TCP


## Model Deployment</code></pre>
</div>
</div>
</div>
</details>
<div class="paragraph">
<p>This runtime can be used in the exact same way as the out of the box ones:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Copy your model files in an object store bucket.</p>
</li>
<li>
<p>Deploy the model from the Dashboard.</p>
</li>
<li>
<p>Make sure you have added a GPU to your GPU configuration, that you have enough VRAM (GPU memory) to load the model, and that you have enough standard memory (RAM). Although the model loads into the GPU, RAM is still used for the pre-loading operations.</p>
</li>
<li>
<p>Once the model is loaded, you can access the inference endpoint provided through the dashboard.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For CPU, note that the model has some limitations such as only being compatible with FP32 and BF16 - any FP16 model gets automatically converted to BF16. Read more about it in the official documentation [here](<a href="https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html" class="bare">https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html</a>).</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_usage"><a class="anchor" href="#_usage"></a>Usage</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This implementation of the runtime provides an <strong>OpenAI compatible API</strong>. So any tool or library that can connect to OpenAI services will be able to consume the endpoint.</p>
</div>
<div class="paragraph">
<p>Python and Curl examples are provided [here](<a href="https://docs.vllm.ai/en/latest/getting_started/quickstart.html#using-openai-completions-api-with-vllm" class="bare">https://docs.vllm.ai/en/latest/getting_started/quickstart.html#using-openai-completions-api-with-vllm</a>).</p>
</div>
<div class="paragraph">
<p>You can also find a notebook example using Langchain to query vLLM in this repo [here](../../examples/notebooks/langchain/Langchain-vLLM-Prompt-memory.ipynb).</p>
</div>
<div class="paragraph">
<p>Also, vLLM provides a full Swagger UI where you can get the full documentation of the API (methods, parameters), and try it directly without any coding,&#8230;&#8203; It is accessible at the address <code><a href="https://your-endpoint-address/docs" class="bare">https://your-endpoint-address/docs</a></code>.</p>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="section2.html">Introduction to vLLM</a></span>
  <span class="next"><a href="section3.html">vLLM Serving Runtime on Red Hat OpenShift AI</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
