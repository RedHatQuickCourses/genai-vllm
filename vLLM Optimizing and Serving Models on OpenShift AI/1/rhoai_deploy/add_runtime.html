<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>vLLM Serving Runtime :: vLLM Optimizing and Serving Models on OpenShift AI</title>
    <link rel="prev" href="rhoai_bench.html">
    <link rel="next" href="rhoai_model.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">vLLM Optimizing and Serving Models on OpenShift AI</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="vLLM Optimizing and Serving Models on OpenShift AI" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../vllm/index.html">vLLM, What is it?</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_intro.html">vLLM</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/rh_ai.html">Red Hat AI Platforms</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_rhoai.html">Integrating vLLM with OpenShift AI: The Serving Runtime</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_deploy.html">vLLM on Red Hat OpenShift AI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../vllm/vllm_concl.html">Summary: Key Takeaways and Next Steps</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../model_sizing/index.html">GPU Architecture and Model Sizing</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/gpu_arch.html">NVIDIA GPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/gpu_cost.html">Estimating GPU VRAM</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/vram_calc.html">Cost-Effective Model Selection</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../rh_hg_ai/index.html">Red Hat AI Model Repository</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/model_types.html">Generative AI Model Variations in Model Naming</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/val_models.html">Validated Models and Quantization Strategies</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/lab_models.html">Pre-Selected Course Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/summary.html">From Curation to Confident Deployment</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">OpenShift AI Configuration</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rhoai_self.html">Optional - RHOAI Self-Managed</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rhoai_221.html">Lab Environment Customization</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rhoai_config.html">Project and Data Connection Setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rhoai_bench.html">Creating the AI Workbench and Preparing Models</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="add_runtime.html">vLLM Serving Runtime</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rhoai_model.html">Deploy Granite LLM on RHOAI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rhoai_query.html">Querying the Deployed Model in a Jupyter Notebook</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="batch_summ.html">Using the AI Model for Batch Summarization</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/further_study.html">Hands on Exploration and Further Study</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../LABENV/index.html">Lab Environments</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/index-lab-demo.html">Red Hat Demo Hub: Lab Environment Selection</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/developer_sandbox.html">Red Hat Developer Platform</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">vLLM Optimizing and Serving Models on OpenShift AI</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a></li>
    <li><a href="index.html">OpenShift AI Configuration</a></li>
    <li><a href="add_runtime.html">vLLM Serving Runtime</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">vLLM Serving Runtime</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>A model-serving runtime provides the necessary integration between Red Hat OpenShift AI and a specific model server. Before deploying our model, we must ensure the <strong>vLLM NVIDIA GPU ServingRuntime</strong> is available in our environment.</p>
</div>
<div class="paragraph">
<p>This page outlines the runtimes included in a recent version of OpenShift AI and provides instructions to add the vLLM runtime manually if you are using an older version.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_default_model_serving_runtimes"><a class="anchor" href="#_default_model_serving_runtimes"></a>Default Model Serving Runtimes</h2>
<div class="sectionbody">
<div class="paragraph">
<p>By default, a modern version of Red Hat OpenShift AI (like v2.21) includes several powerful runtimes out-of-the-box:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Multi-model</strong></p>
<div class="ulist">
<ul>
<li>
<p>OpenVINO Model Server</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Single-model</strong></p>
<div class="ulist">
<ul>
<li>
<p>OpenVINO Model Server</p>
</li>
<li>
<p>Caikit Standalone ServingRuntime for KServe</p>
</li>
<li>
<p>Caikit TGIS ServingRuntime for KServe</p>
</li>
<li>
<p>vLLM NVIDIA GPU ServingRuntime for KServe</p>
</li>
<li>
<p>vLLM Intel Gaudi Accelerator ServingRuntime for KServe</p>
</li>
<li>
<p>vLLM AMD GPU ServingRuntime for KServe</p>
</li>
<li>
<p>vLLM CPU ServingRuntime for KServe</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>If the <strong>vLLM NVIDIA GPU ServingRuntime for KServe</strong> is already available when you go to deploy a model, you do not need to complete the following exercise and can proceed to the next section.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>While OpenShift AI supports the ability to add your own runtime, you are responsible for configuring, adjusting, and maintaining any custom runtimes you deploy.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_exercise_add_the_vllm_custom_runtime"><a class="anchor" href="#_exercise_add_the_vllm_custom_runtime"></a>Exercise: Add the vLLM Custom Runtime</h2>
<div class="sectionbody">
<div class="paragraph">
<p>If your environment does not include the vLLM runtime by default, this exercise will guide you through the steps to add it as a custom runtime.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you are using OpenShift AI v2.21 or newer, adding this runtime using the exact name below will fail, as it is already built-in. This exercise is intended for older versions.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Log in to the OpenShift AI dashboard with an administrator account.</p>
</li>
<li>
<p>In the left navigation menu, navigate to <strong>Settings</strong> &#8594; <strong>Serving runtimes</strong>.</p>
</li>
<li>
<p>Click the <strong>Add serving runtime</strong> button.</p>
</li>
<li>
<p>On the "Add model-serving runtime" page, make the following selections:</p>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p><strong>Model-serving platform:</strong> Select <strong>Single-model serving platform</strong>.</p>
</li>
<li>
<p><strong>API protocol:</strong> Select <strong>REST</strong>.</p>
</li>
</ul>
</div>
</div>
</div>
</li>
<li>
<p>Click the <strong>Start from scratch</strong> radio button.</p>
</li>
<li>
<p>In the YAML editor that appears, paste the following configuration:</p>
<div class="listingblock">
<div class="title">vllm-servingruntime.yaml</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># Filename: vllm-servingruntime.yaml
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
 name: vllm-cuda-runtime-example
 annotations:
   openshift.io/display-name: vLLM (NVIDIA GPU)
   opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
 labels:
   opendatahub.io/dashboard: 'true'
spec:
 supportedModelFormats:
   - name: vLLM
     autoSelect: true
 containers:
   - name: kserve-container
     image: quay.io/modh/vllm:rhoai-2.20-cuda
     command: ["python", "-m", "vllm.entrypoints.openai.api_server"]
     args:
       - "--port=8080"
       - "--model=/mnt/models"
       - "--served-model-name={{.Name}}"
     env:
       - name: HF_HOME
         value: /tmp/hf_home
     ports:
       - containerPort: 8080
         protocol: TCP
     resources:
       requests:
         nvidia.com/gpu: '1'
       limits:
         nvidia.com/gpu: '1'</code></pre>
</div>
</div>
</li>
<li>
<p>After pasting the YAML, click the <strong>Create</strong> button.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>You will now see the new <strong>vLLM (NVIDIA GPU)</strong> runtime in the list of available serving runtimes.</p>
</div>
<hr>
<div class="paragraph">
<p>With the vLLM runtime now available in your cluster, the next step is to configure a storage location for our models. We will use MinIO, an S3-compatible object store, for this lab.</p>
</div>
<div class="paragraph">
<p>Continue to the next section to deploy and configure MinIO.</p>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="rhoai_bench.html">Creating the AI Workbench and Preparing Models</a></span>
  <span class="next"><a href="rhoai_model.html">Deploy Granite LLM on RHOAI</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
