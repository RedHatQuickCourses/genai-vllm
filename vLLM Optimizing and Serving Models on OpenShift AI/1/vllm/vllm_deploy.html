<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>vLLM on Red Hat OpenShift AI :: vLLM Optimizing and Serving Models on OpenShift AI</title>
    <link rel="prev" href="vllm_rhoai.html">
    <link rel="next" href="vllm_concl.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">vLLM Optimizing and Serving Models on OpenShift AI</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="vLLM Optimizing and Serving Models on OpenShift AI" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">vLLM, What is it?</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="vllm_intro.html">vLLM</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rh_ai.html">Red Hat AI Platforms</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="vllm_rhoai.html">Integrating vLLM with OpenShift AI: The Serving Runtime</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="vllm_deploy.html">vLLM on Red Hat OpenShift AI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="vllm_concl.html">Summary: Key Takeaways and Next Steps</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../model_sizing/index.html">GPU Architecture and Model Sizing</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/gpu_arch.html">NVIDIA GPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/gpu_cost.html">Estimating GPU VRAM</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/vram_calc.html">Cost-Effective Model Selection</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../rh_hg_ai/index.html">Red Hat AI Model Repository</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/model_types.html">Generative AI Model Variations in Model Naming</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/val_models.html">Validated Models and Quantization Strategies</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/lab_models.html">Pre-Selected Course Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/summary.html">From Curation to Confident Deployment</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter7/index.html">OpenShift AI Configuration</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter7/section1.html">Creating OpenShift AI Resources - 1</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter7/section2.html">MinIO S3 Compatible Storage Setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter7/section3.html">OpenShift AI Resources - 2</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter8/index.html">Jupyter Notebooks &amp; LLMs</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter8/section1.html">Jupyter Notebooks</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter8/section2.html">Mistral LLM Model Inference</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter8/section3.html">Llama3 LLM Model Inference</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter9/index.html">blank</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter9/section4.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter9/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter9/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter9/minios3.html">MinIO S3: Compatible Storage Deployment</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/model_phases.html">Common Generative AI Model Variations</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../appendix/further_study.html">Hands on Exploration and Further Study</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../LABENV/index.html">Lab Environments</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/index-lab-demo.html">Red Hat Demo Hub: Lab Environment Selection</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/developer_sandbox.html">Red Hat Developer Platform</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">vLLM Optimizing and Serving Models on OpenShift AI</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a></li>
    <li><a href="index.html">vLLM, What is it?</a></li>
    <li><a href="vllm_deploy.html">vLLM on Red Hat OpenShift AI</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">vLLM on Red Hat OpenShift AI</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Now that we understand what a Serving Runtime is, let&#8217;s walk through the practical considerations for deploying a Large Language Model using our vLLM runtime on Red Hat OpenShift AI. This module covers the end-to-end workflow, from resource planning to interacting with the final API endpoint.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_deployment_workflow_at_a_glance"><a class="anchor" href="#_the_deployment_workflow_at_a_glance"></a>The Deployment Workflow at a Glance</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The process of taking a model and making it live with the vLLM runtime on OpenShift AI can be broken down into four main stages:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Choose the Compute Target:</strong> Decide whether to deploy on a GPU (for production) or a CPU (for specific, non-performant use cases).</p>
</li>
<li>
<p><strong>Prepare Model Artifacts:</strong> Place the model files in an S3-compatible object storage bucket that OpenShift AI can access.</p>
</li>
<li>
<p><strong>Deploy via the Dashboard:</strong> Use the OpenShift AI interface to configure the deployment, selecting the vLLM runtime and allocating the necessary resources.</p>
</li>
<li>
<p><strong>Test and Integrate:</strong> Verify the deployment using the built-in API documentation and integrate it into applications using the OpenAI-compatible endpoint.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>We will now explore the key considerations for each of these stages.</p>
</div>
<div class="sect2">
<h3 id="_consideration_1_choosing_the_right_compute_target_gpu_vs_cpu"><a class="anchor" href="#_consideration_1_choosing_the_right_compute_target_gpu_vs_cpu"></a>Consideration 1: Choosing the Right Compute Target (GPU vs. CPU)</h3>
<div class="paragraph">
<p>Your first decision is where the model will run. While vLLM can operate in both environments, they serve very different purposes.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 20%;">
<col style="width: 40%;">
<col style="width: 40%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Feature</th>
<th class="tableblock halign-left valign-top">GPU-Accelerated vLLM (Production Standard)</th>
<th class="tableblock halign-left valign-top">CPU-Only vLLM (Niche Use Cases)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Hardware Requirement</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">NVIDIA GPU is <strong>mandatory</strong>.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Runs on standard x86 CPUs.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Performance</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">High-throughput and low-latency. Optimized with custom CUDA kernels for parallel processing.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Significantly slower. Suitable only for development, testing, or low-load applications with small models.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Key Limitation</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Requires availability of expensive GPU resources and sufficient VRAM.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Limited data type support. It primarily works with <code>FP32</code> and <code>BF16</code>, and will automatically convert <code>FP16</code> models, which can affect performance and accuracy.</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>For any production, pre-production, or performance-sensitive workload, <strong>always use the GPU-accelerated runtime</strong>. The CPU runtime should be reserved for scenarios where GPU resources are completely unavailable or for quick functional testing where performance is not a factor.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_consideration_2_the_model_deployment_process"><a class="anchor" href="#_consideration_2_the_model_deployment_process"></a>Consideration 2: The Model Deployment Process</h3>
<div class="paragraph">
<p>Deploying the model involves telling OpenShift AI where to find the model files and how to configure the vLLM server to run them.</p>
</div>
<div class="sect3">
<h4 id="_step_1_stage_model_files_in_object_storage"><a class="anchor" href="#_step_1_stage_model_files_in_object_storage"></a>Step 1: Stage Model Files in Object Storage</h4>
<div class="paragraph">
<p>The OpenShift AI model serving platform is designed to pull model artifacts from an S3-compatible object store. Before starting a deployment, ensure your model files are uploaded to a bucket and that you have a <code>Data Connection</code> configured in your OpenShift AI project to access it.</p>
</div>
</div>
<div class="sect3">
<h4 id="_step_2_deploy_from_the_openshift_ai_dashboard"><a class="anchor" href="#_step_2_deploy_from_the_openshift_ai_dashboard"></a>Step 2: Deploy from the OpenShift AI Dashboard</h4>
<div class="paragraph">
<p>With the <code>vLLM ServingRuntime</code> registered by an administrator (as covered in Module 1), the deployment process is straightforward:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Navigate to your project in the OpenShift AI dashboard and click <strong>Deploy model</strong>.</p>
</li>
<li>
<p>Select the <strong>vLLM (NVIDIA GPU)</strong> runtime you intend to use.</p>
</li>
<li>
<p>Point to the <strong>Data Connection</strong> and the specific path within the bucket where your model files are located.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_step_3_critical_resource_allocation"><a class="anchor" href="#_step_3_critical_resource_allocation"></a>Step 3: Critical Resource Allocation</h4>
<div class="paragraph">
<p>This is the most critical step for a successful deployment. You must allocate enough memory for the model to load and run.</p>
</div>
<div class="sidebarblock">
<div class="content">
<div class="title">VRAM vs. System RAM</div>
<div class="ulist">
<ul>
<li>
<p><strong>GPU VRAM (Video RAM):</strong> This is the memory on the GPU itself. The <strong>entire model</strong> and its KV Cache must fit into VRAM. If you do not allocate enough VRAM, the model will fail to load. Check the size of your model (e.g., a 7B parameter model in FP16 requires ~14 GB of VRAM) and allocate accordingly.</p>
</li>
<li>
<p><strong>System RAM:</strong> This is the standard server memory. While the model runs on the GPU, system RAM is still used by the operating system and for pre-loading operations before the model is transferred to the GPU. Always allocate a healthy amount of system RAM (e.g., 16 GB or more) to prevent unexpected pod crashes.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_consideration_3_interacting_with_the_deployed_model"><a class="anchor" href="#_consideration_3_interacting_with_the_deployed_model"></a>Consideration 3: Interacting with the Deployed Model</h3>
<div class="paragraph">
<p>A key advantage of the vLLM runtime is its built-in, <strong>OpenAI-compatible API</strong>. This design choice dramatically simplifies integration, as any application or tool built to communicate with OpenAI&#8217;s API can be pointed to your self-hosted vLLM endpoint with minimal changes.</p>
</div>
<div class="sect3">
<h4 id="_interactive_testing_with_the_swagger_ui"><a class="anchor" href="#_interactive_testing_with_the_swagger_ui"></a>Interactive Testing with the Swagger UI</h4>
<div class="paragraph">
<p>The easiest way to test your deployed model is through the built-in Swagger UI, which provides interactive API documentation. You can see all available endpoints, view their parameters, and even send test requests directly from your browser.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>How to Access:</strong> Simply take the inference endpoint URL provided by the OpenShift AI dashboard and append <code>/docs</code> to it.</p>
</li>
<li>
<p><strong>URL:</strong> <code><a href="https://&lt;your-model-endpoint-url&gt;/docs" class="bare">https://&lt;your-model-endpoint-url&gt;/docs</a></code></p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_programmatic_integration"><a class="anchor" href="#_programmatic_integration"></a>Programmatic Integration</h4>
<div class="paragraph">
<p>For application development, you can use standard HTTP clients or libraries like <code>openai-python</code>. The vLLM server implements the core OpenAI endpoints:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>/v1/chat/completions</code>: For interacting with conversational or instruction-tuned models.</p>
</li>
<li>
<p><code>/v1/completions</code>: For legacy text-completion models.</p>
</li>
<li>
<p><code>/v1/embeddings</code>: For generating vector embeddings from text.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Example: <code>curl</code> Request</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># 1. Set your model's endpoint URL and name
export ENDPOINT_URL="https://&lt;your-model-endpoint-url&gt;/v1/chat/completions"
export MODEL_NAME="name-of-your-deployed-model" # e.g., "Meta-Llama-3-8B-Instruct"

# 2. Send the POST request
curl -s "$ENDPOINT_URL" \
-H "Content-Type: application/json" \
-d @- &lt;&lt; EOF
{
  "model": "$MODEL_NAME",
  "messages": [{"role": "user", "content": "Explain the role of VRAM in LLM inference."}],
  "temperature": 0.7
}
EOF</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
For more advanced use cases, such as integrating vLLM with frameworks like LangChain, you can find detailed examples and notebooks in the official vLLM and OpenShift AI documentation. The key is to configure your client to use your model&#8217;s inference endpoint as the <code>base_url</code>.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="vllm_rhoai.html">Integrating vLLM with OpenShift AI: The Serving Runtime</a></span>
  <span class="next"><a href="vllm_concl.html">Summary: Key Takeaways and Next Steps</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
