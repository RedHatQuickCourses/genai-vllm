<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>vLLM and Red Hat AI Platforms :: vLLM Optimizing and Serving Models on OpenShift AI</title>
    <link rel="prev" href="index.html">
    <link rel="next" href="vllm_rhoai.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">vLLM Optimizing and Serving Models on OpenShift AI</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="genai-vllm" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../LABENV/index.html">Lab Environments</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/index-lab-demo.html">Red Hat Demo Hub: Lab Environment Selection</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/developer_sandbox.html">Red Hat Developer Platform</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">vLLM Overview</a>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="vllm_intro.html">vLLM and Red Hat AI Platforms</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="vllm_rhoai.html">Integrating vLLM with OpenShift AI: The Serving Runtime</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="vllm_deploy.html">Deploying and Interacting with Models on OpenShift AI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="vllm_concl.html">vLLM Technical Deep Dive and Advanced Capabilities</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../model_sizing/index.html">GPU Architecture and Model Sizing</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/gpu_cost.html">Estimating GPU VRAM</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/gpu_arch.html">Optimizing with NVIDIA GPU Architecture</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../rh_hg_ai/index.html">Red Hat AI Model Repository</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/model_types.html">The Red Hat AI Validated Model Repository</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/val_models.html">Intro Quantization Strategies</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/lab_models.html">Granite Models</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../rhoai_deploy/index.html">OpenShift AI Configuration</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_self.html">Optional - RHOAI Self-Managed</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_221.html">Lab Environment Customization</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_config.html">Project and Data Connection Setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_bench.html">Creating the AI Workbench and Preparing Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/add_runtime.html">vLLM Serving Runtime</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_model.html">Deploy Granite LLM on RHOAI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_query.html">Querying the Deployed Model in a Jupyter Notebook</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/batch_summ.html">Using the AI Model for Batch Summarization</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/guide_llm.html">Performance Benchmarking with GuideLLM</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">vLLM Optimizing and Serving Models on OpenShift AI</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a></li>
    <li><a href="index.html">vLLM Overview</a></li>
    <li><a href="vllm_intro.html">vLLM and Red Hat AI Platforms</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">vLLM and Red Hat AI Platforms</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>This section sets the stage by defining vLLM and contextualizing it within Red Hat&#8217;s AI ecosystem.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_what_is_vllm"><a class="anchor" href="#_what_is_vllm"></a>What is vLLM?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>vLLM is an open-source <strong>inference engine</strong> designed for serving Large Language Models (LLMs) with maximum speed and memory efficiency. Developed by researchers at UC Berkeley, vLLM is not a model itself, but a highly optimized library that runs LLMs, making them available for applications.</p>
</div>
<div class="paragraph">
<p>Its primary goal is to solve the key challenges of LLM inference: high latency, low throughput, and demanding GPU memory requirements. By addressing these bottlenecks, vLLM enables organizations to serve more users at a lower cost.</p>
</div>
<div class="sidebarblock">
<div class="content">
<div class="title">The Core Problem vLLM Solves</div>
<div class="paragraph">
<p>Serving LLMs is memory-intensive, primarily due to the <strong>Key-Value (KV) Cache</strong>. This cache stores intermediate attention data for each user&#8217;s request. Traditional serving methods allocate large, contiguous blocks of memory for this cache, leading to significant waste and limiting the number of concurrent requests a single GPU can handle. vLLM&#8217;s core innovations directly target this inefficiency.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_why_vllm_the_key_benefits"><a class="anchor" href="#_why_vllm_the_key_benefits"></a>Why vLLM? The Key Benefits</h3>
<div class="paragraph">
<p>For platform engineers and consultants, the choice of a serving engine has direct impacts on performance, cost, and maintainability. vLLM provides compelling advantages.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>High-Performance Throughput
vLLM can increase the number of output tokens generated per second by <strong>2-4x</strong> compared to standard Hugging Face implementations, without any model architecture changes. This means a single GPU can serve significantly more users concurrently.</p>
</li>
<li>
<p>Efficient Memory Management
Through its core innovation, PagedAttention, vLLM drastically reduces the memory footprint of the KV Cache by up to <strong>55%</strong>. This allows you to:</p>
<div class="ulist">
<ul>
<li>
<p>Run larger models on the same GPU.</p>
</li>
<li>
<p>Fit more concurrent requests into memory, boosting throughput.</p>
</li>
<li>
<p>Reduce overall hardware costs.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Seamless Integration with the OpenAI API
vLLM provides an API server that is a <strong>drop-in replacement</strong> for the OpenAI API. This is a critical feature for adoption, as applications built using the <code>openai</code> client library can be pointed to a self-hosted vLLM endpoint with minimal to no code changes.</p>
</li>
</ol>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The OpenAI-compatible API means you can develop applications against OpenAI&#8217;s service and later migrate to a self-hosted, cost-effective vLLM instance without rewriting your application logic. This provides immense flexibility for prototyping and production deployment.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_red_hat_ai_platform_context"><a class="anchor" href="#_red_hat_ai_platform_context"></a>Red Hat AI Platform Context</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Red Hat AI provides a supported, enterprise version of open source tools for the entire AI lifecycle, offering flexibility to train, tune, deploy, and run models on-premise, in the public cloud, or at the edge.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"><strong>Red Hat AI Inference Server</strong></th>
<th class="tableblock halign-left valign-top"><strong>Red Hat Enterprise Linux AI</strong></th>
<th class="tableblock halign-left valign-top"><strong>Red Hat OpenShift AI</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Powered by vLLM, it optimizes model inference and includes access to validated/optimized third-party models and LLM compressor tools.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Red Hat Enterprise Linux® AI is a platform for inference and training of large language models to power enterprise applications. It includes InstructLab tooling for customizing models, as well as integrated hardware accelerators.</p>
<p class="tableblock">+ Includes Red Hat AI Inference Server</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">This platform, built on Red Hat OpenShift, manages the lifecycle of generative and predictive AI models at scale. It integrates <strong>MLOps and LLMOps capabilities</strong> for complete lifecycle management, including distributed training, tuning, inference, and monitoring across hybrid cloud environments.</p>
<p class="tableblock">+ Includes Red Hat AI Inference Server.</p>
<p class="tableblock">+ Includes Red Hat Enterprise Linux AI.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect1">
<h2 id="_vllms_role_in_red_hat_ai"><a class="anchor" href="#_vllms_role_in_red_hat_ai"></a>vLLM&#8217;s Role in Red Hat AI</h2>
<div class="sectionbody">
<div class="paragraph">
<p>vLLM serves as the inference engine for Red Hat AI Inference Server, Red Hat Enterprise Linux AI, and supports Kubernetes via KServe for OpenShift AI inference use cases. It is designed to run efficiently on various hardware configurations and supports breaking up processing work across multiple GPUs for efficient resource utilization. Red Hat AI also supports disconnected and air-gapped environments for sensitive data.</p>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="index.html">vLLM Overview</a></span>
  <span class="next"><a href="vllm_rhoai.html">Integrating vLLM with OpenShift AI: The Serving Runtime</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
