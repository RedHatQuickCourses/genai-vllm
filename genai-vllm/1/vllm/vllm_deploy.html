<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Deploying and Interacting with Models on OpenShift AI :: vLLM Optimizing and Serving Models on OpenShift AI</title>
    <link rel="prev" href="vllm_rhoai.html">
    <link rel="next" href="vllm_concl.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">vLLM Optimizing and Serving Models on OpenShift AI</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="genai-vllm" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../LABENV/index.html">Lab Environments</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/index-lab-demo.html">Red Hat Demo Hub: Lab Environment Selection</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/developer_sandbox.html">Red Hat Developer Platform</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">vLLM Overview</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="vllm_intro.html">vLLM and Red Hat AI Platforms</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="vllm_rhoai.html">Integrating vLLM with OpenShift AI: The Serving Runtime</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="vllm_deploy.html">Deploying and Interacting with Models on OpenShift AI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="vllm_concl.html">vLLM Technical Deep Dive and Advanced Capabilities</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../model_sizing/index.html">GPU Architecture and Model Sizing</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/gpu_cost.html">Estimating GPU VRAM</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../model_sizing/gpu_arch.html">Optimizing with NVIDIA GPU Architecture</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../rh_hg_ai/index.html">Red Hat AI Model Repository</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/model_types.html">The Red Hat AI Validated Model Repository</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/val_models.html">Intro Quantization Strategies</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rh_hg_ai/lab_models.html">Granite Models</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../rhoai_deploy/index.html">OpenShift AI Configuration</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_self.html">Optional - RHOAI Self-Managed</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_221.html">Lab Environment Customization</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_config.html">Project and Data Connection Setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_bench.html">Creating the AI Workbench and Preparing Models</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/add_runtime.html">vLLM Serving Runtime</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_model.html">Deploy Granite LLM on RHOAI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/rhoai_query.html">Querying the Deployed Model in a Jupyter Notebook</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/batch_summ.html">Using the AI Model for Batch Summarization</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rhoai_deploy/guide_llm.html">Performance Benchmarking with GuideLLM</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">vLLM Optimizing and Serving Models on OpenShift AI</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">vLLM Optimizing and Serving Models on OpenShift AI</a></li>
    <li><a href="index.html">vLLM Overview</a></li>
    <li><a href="vllm_deploy.html">Deploying and Interacting with Models on OpenShift AI</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Deploying and Interacting with Models on OpenShift AI</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>This page details the practical, step-by-step workflow for deploying and interacting with models, emphasizing resource allocation which is vital for delivery engineers.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_model_deployment_workflow_end_to_end"><a class="anchor" href="#_the_model_deployment_workflow_end_to_end"></a>The Model Deployment Workflow (End-to-End):</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Register the Runtime (Admin Task):</strong>
An administrator with cluster privileges must first install necessary OpenShift AI components. Then, they add the vLLM runtime by navigating to the OpenShift AI Dashboard, going to Settings → Serving runtimes, clicking Add serving runtime, choosing Single model serving platform, and pasting the vllm-servingruntime.yaml content. The "vLLM (NVIDIA GPU)" runtime will then be available.</p>
</li>
<li>
<p><strong>Prepare Model Artifacts:</strong> Model files (e.g., from Hugging Face) must be located in an S3-compatible object storage bucket that OpenShift AI can access. A Data Connection pointing to this bucket must be created in the OpenShift AI project.</p>
</li>
<li>
<p><strong>Deploy a Model (User Task):</strong></p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>From the OpenShift AI project dashboard, click Deploy model.</p>
</li>
<li>
<p>Fill in the model name, select the vLLM (NVIDIA GPU) serving runtime, and point to the data connection and specific model path.</p>
</li>
<li>
<p>Critical Resource Allocation: This is the most critical step for successful deployment.</p>
<div class="olist lowerroman">
<ol class="lowerroman" type="i">
<li>
<p>GPU VRAM (Video RAM): This is the memory on the GPU itself. The entire model and its KV Cache must fit into VRAM. If insufficient VRAM is allocated, the model will fail to load. For example, a 7B parameter model in FP16 requires approximately 14 GB of VRAM.
System RAM: Used by the operating system and for pre-loading operations before the model is transferred to the GPU. Allocate a healthy amount of system RAM (e.g., 16 GB or more) to prevent unexpected pod crashes and ensure stability during startup.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Configure the number of replicas and ensure a GPU is requested in the resource configuration.</p>
</li>
<li>
<p>OpenShift AI will then automatically provision a pod using the vLLM runtime, pull the model from object storage into the pod’s /mnt/models directory, and start the vLLM server.</p>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p><strong>Choosing the Right Compute Target (GPU vs. CPU)</strong></p>
</div>
<div class="paragraph">
<p>Your first decision is where the model will run. While vLLM can operate in both environments, they serve very different purposes.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 20%;">
<col style="width: 40%;">
<col style="width: 40%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Feature</th>
<th class="tableblock halign-left valign-top">GPU-Accelerated vLLM (Production Standard)</th>
<th class="tableblock halign-left valign-top">CPU-Only vLLM (Niche Use Cases)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Hardware Requirement</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">(NVIDIA) GPU is <strong>mandatory</strong>.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Runs on standard x86 CPUs.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Performance</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">High-throughput and low-latency. Optimized with custom CUDA kernels for parallel processing.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Significantly slower. Suitable only for development, testing, or low-load applications with small models.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Key Limitation</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Requires availability of expensive GPU resources and sufficient VRAM.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Limited data type support. It primarily works with <code>FP32</code> and <code>BF16</code>, and will automatically convert <code>FP16</code> models, which can affect performance and accuracy.</p></td>
</tr>
</tbody>
</table>
<div class="sidebarblock">
<div class="content">
<div class="title">VRAM vs. System RAM</div>
<div class="ulist">
<ul>
<li>
<p><strong>GPU VRAM (Video RAM):</strong> This is the memory on the GPU itself. The <strong>entire model</strong> and its KV Cache must fit into VRAM. If you do not allocate enough VRAM, the model will fail to load. Check the size of your model (e.g., a 7B parameter model in FP16 requires ~14 GB of VRAM) and allocate accordingly.</p>
</li>
<li>
<p><strong>System RAM:</strong> This is the standard server memory. While the model runs on the GPU, system RAM is still used by the operating system and for pre-loading operations before the model is transferred to the GPU. Always allocate a healthy amount of system RAM (e.g., 16 GB or more) to prevent unexpected pod crashes.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_interacting_with_the_deployed_model"><a class="anchor" href="#_interacting_with_the_deployed_model"></a>Interacting with the Deployed Model:</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A key advantage of the vLLM runtime is its built-in, <strong>OpenAI-compatible API</strong>. This design choice dramatically simplifies integration, as any application or tool built to communicate with OpenAI&#8217;s API can be pointed to your self-hosted vLLM endpoint with minimal changes.</p>
</div>
<div class="sect2">
<h3 id="_interactive_testing_with_the_swagger_ui"><a class="anchor" href="#_interactive_testing_with_the_swagger_ui"></a>Interactive Testing with the Swagger UI</h3>
<div class="paragraph">
<p>The easiest way to test your deployed model is through the built-in Swagger UI, which provides interactive API documentation. You can see all available endpoints, view their parameters, and even send test requests directly from your browser.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>How to Access:</strong> Simply take the inference endpoint URL provided by the OpenShift AI dashboard and append <code>/docs</code> to it.</p>
</li>
<li>
<p><strong>URL:</strong> <code><a href="https://&lt;your-model-endpoint-url&gt;/docs" class="bare">https://&lt;your-model-endpoint-url&gt;/docs</a></code></p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_programmatic_integration"><a class="anchor" href="#_programmatic_integration"></a>Programmatic Integration</h3>
<div class="paragraph">
<p>For application development, you can use standard HTTP clients or libraries like <code>openai-python</code>. The vLLM server implements the core OpenAI endpoints:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>/v1/chat/completions</code>: For interacting with conversational or instruction-tuned models.</p>
</li>
<li>
<p><code>/v1/completions</code>: For legacy text-completion models.</p>
</li>
<li>
<p><code>/v1/embeddings</code>: For generating vector embeddings from text.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Example: <code>curl</code> Request</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># 1. Set your model's endpoint URL and name
export ENDPOINT_URL="https://&lt;your-model-endpoint-url&gt;/v1/chat/completions"
export MODEL_NAME="name-of-your-deployed-model" # e.g., "Meta-Llama-3-8B-Instruct"

# 2. Send the POST request
curl -s "$ENDPOINT_URL" \
-H "Content-Type: application/json" \
-d @- &lt;&lt; EOF
{
  "model": "$MODEL_NAME",
  "messages": [{"role": "user", "content": "Explain the role of VRAM in LLM inference."}],
  "temperature": 0.7
}
EOF</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
For more advanced use cases, such as integrating vLLM with frameworks like LangChain, you can find  example notebooks in this courses Github Apps repository and in OpenShift AI documentation. The key is to configure your client to use your model&#8217;s inference endpoint as the <code>base_url</code>.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="vllm_rhoai.html">Integrating vLLM with OpenShift AI: The Serving Runtime</a></span>
  <span class="next"><a href="vllm_concl.html">vLLM Technical Deep Dive and Advanced Capabilities</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
